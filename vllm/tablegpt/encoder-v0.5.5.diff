diff --git a/vllm/entrypoints/chat_utils.py b/vllm/entrypoints/chat_utils.py
index 19d10950..a39b8ec4 100644
--- a/vllm/entrypoints/chat_utils.py
+++ b/vllm/entrypoints/chat_utils.py
@@ -1,9 +1,21 @@
 import codecs
+import re
+import random
+import inspect
 from dataclasses import dataclass
-from functools import lru_cache
+from functools import lru_cache, partial
 from pathlib import Path
-from typing import (Any, Awaitable, Iterable, List, Literal, Optional, Tuple,
-                    Union)
+from typing import (
+    Any,
+    Awaitable,
+    Iterable,
+    List,
+    Literal,
+    Optional,
+    Tuple,
+    Union,
+    cast,
+)
 
 # yapf conflicts with isort for this block
 # yapf: disable
@@ -13,16 +25,20 @@ from openai.types.chat import (
 from openai.types.chat import ChatCompletionContentPartTextParam
 from openai.types.chat import (
     ChatCompletionMessageParam as OpenAIChatCompletionMessageParam)
+from openai.types.chat import ChatCompletionMessageToolCallParam
 # yapf: enable
 # pydantic needs the TypedDict from typing_extensions
 from pydantic import ConfigDict, TypeAdapter
-from typing_extensions import Required, TypeAlias, TypedDict
+from typing_extensions import Required, TypedDict
 
 from vllm.config import ModelConfig
 from vllm.logger import init_logger
 from vllm.multimodal import MultiModalDataDict
-from vllm.multimodal.utils import (async_get_and_parse_audio,
-                                   async_get_and_parse_image)
+from vllm.multimodal.base import ColumnsTable, MarkdownTable
+from vllm.multimodal.utils import (
+    async_get_and_parse_audio,
+    async_get_and_parse_image,
+)
 from vllm.transformers_utils.tokenizer import AnyTokenizer
 
 logger = init_logger(__name__)
@@ -49,13 +65,24 @@ class CustomChatCompletionContentPartParam(TypedDict, total=False):
     """The type of the content part."""
 
 
-ChatCompletionContentPartParam: TypeAlias = Union[
-    OpenAIChatCompletionContentPartParam, ChatCompletionContentPartAudioParam,
-    CustomChatCompletionContentPartParam, ]
+class ChatCompletionContentPartTabularParam(TypedDict, total=False):
+    tables: Required[Union[List[ColumnsTable] | List[MarkdownTable]]]
+
+    type: Required[Literal["table"]]
+    """The type of the content part."""
+
+
+ChatCompletionContentPartParam = Union[
+    OpenAIChatCompletionContentPartParam,
+    ChatCompletionContentPartAudioParam,
+    ChatCompletionContentPartTabularParam,
+    CustomChatCompletionContentPartParam,
+]
 
 
 class CustomChatCompletionMessageParam(TypedDict, total=False):
     """Enables custom roles in the Chat Completion API."""
+
     role: Required[str]
     """The role of the message's author."""
 
@@ -69,9 +96,16 @@ class CustomChatCompletionMessageParam(TypedDict, total=False):
     same role.
     """
 
+    tool_call_id: Optional[str]
+    """Tool call that this message is responding to."""
+
+    tool_calls: Optional[Iterable[ChatCompletionMessageToolCallParam]]
+    """The tool calls generated by the model, such as function calls."""
 
-ChatCompletionMessageParam = Union[OpenAIChatCompletionMessageParam,
-                                   CustomChatCompletionMessageParam]
+
+ChatCompletionMessageParam = Union[
+    OpenAIChatCompletionMessageParam, CustomChatCompletionMessageParam
+]
 
 
 # TODO: Make fields ReadOnly once mypy supports it
@@ -83,11 +117,12 @@ class ConversationMessage(TypedDict):
 @dataclass(frozen=True)
 class ChatMessageParseResult:
     messages: List[ConversationMessage]
-    mm_futures: List[Awaitable[MultiModalDataDict]]
+    mm_futures: List[Union[Awaitable[MultiModalDataDict], MultiModalDataDict]]
 
 
 def load_chat_template(
-        chat_template: Optional[Union[Path, str]]) -> Optional[str]:
+    chat_template: Optional[Union[Path, str]],
+) -> Optional[str]:
     if chat_template is None:
         return None
     try:
@@ -99,9 +134,11 @@ def load_chat_template(
 
         JINJA_CHARS = "{}\n"
         if not any(c in chat_template for c in JINJA_CHARS):
-            msg = (f"The supplied chat template ({chat_template}) "
-                   f"looks like a file path, but it failed to be "
-                   f"opened. Reason: {e}")
+            msg = (
+                f"The supplied chat template ({chat_template}) "
+                f"looks like a file path, but it failed to be "
+                f"opened. Reason: {e}"
+            )
             raise ValueError(msg) from e
 
         # If opening a file fails, set chat template to be args to
@@ -113,8 +150,11 @@ def load_chat_template(
 
 
 @lru_cache(maxsize=None)
-def _mm_token_str(model_config: ModelConfig, tokenizer: AnyTokenizer,
-                  modality: Literal["image", "audio"]) -> Optional[str]:
+def _mm_token_str(
+    model_config: ModelConfig,
+    tokenizer: AnyTokenizer,
+    modality: Literal["image", "audio", "tabular"],
+) -> Optional[str]:
     # TODO: Let user specify how to insert image tokens into prompt
     # (similar to chat template)
     model_type = model_config.hf_config.model_type
@@ -137,24 +177,126 @@ def _mm_token_str(model_config: ModelConfig, tokenizer: AnyTokenizer,
         if model_type == "ultravox":
             return "<|reserved_special_token_0|>"
         raise TypeError(f"Unknown model type: {model_type}")
+    elif modality == "tabular":
+        if model_type == "tablegpt_markup":
+            return model_config.hf_config.placeholder_token
+        if model_type == "tablegpt_contrastive":
+            return None
     else:
         raise TypeError(f"Unknown modality: {modality}")
 
 
 # TODO: Let user specify how to insert multimodal tokens into prompt
 # (similar to chat template)
-def _get_full_multimodal_text_prompt(placeholder_token_str: str,
-                                     text_prompt: str) -> str:
+def _get_full_multimodal_text_prompt(
+    placeholder_token_str: str, text_prompt: str, text_prompt_prefix=False
+) -> str:
     """Combine multimodal prompts for a multimodal language model"""
 
     # NOTE: For now we assume all model architectures use the same
     # placeholder + text prompt format. This may change in the future.
-    return f"{placeholder_token_str}\n{text_prompt}"
+    return (
+        f"{text_prompt}\n{placeholder_token_str}"
+        if text_prompt_prefix
+        else f"{placeholder_token_str}\n{text_prompt}"
+    )
+
+
+def _get_full_tarbular_text_prompt(
+    placeholder_token_str: str, table_infos: List[str], text_prompt: str
+) -> str:
+    occurrences = re.findall(re.escape(placeholder_token_str), text_prompt)
+    if len(occurrences) == 0:
+        return _get_full_multimodal_text_prompt(
+            placeholder_token_str="".join(table_infos),
+            text_prompt=text_prompt,
+            text_prompt_prefix=True,
+        )
+
+    if len(occurrences) != len(table_infos):
+        raise ValueError(
+            f"The length of table_infos::List"
+            f"must match the number of occurrences of "
+            f"`{placeholder_token_str}`."
+        )
+
+    return re.sub(
+        re.escape(placeholder_token_str),
+        lambda match: table_infos.pop(0),
+        text_prompt,
+    )
 
 
 _TextParser = TypeAdapter(ChatCompletionContentPartTextParam)
 _ImageParser = TypeAdapter(ChatCompletionContentPartImageParam)
 _AudioParser = TypeAdapter(ChatCompletionContentPartAudioParam)
+_TabularParser = TypeAdapter(ChatCompletionContentPartTabularParam)
+
+
+def __dataframe_info_simple(
+    table: ColumnsTable | MarkdownTable, model_config: ModelConfig
+) -> str:
+    
+    if isinstance(table, MarkdownTable):
+        return model_config.hf_config.placeholder_token + "\n"
+
+    max_example_values = 3
+
+    insert_embs_token = model_config.hf_config.encoder_config.insert_embs_token
+    insert_seq_token = model_config.hf_config.encoder_config.insert_seq_token
+
+    placeholder_val = insert_seq_token + insert_embs_token + insert_seq_token
+    
+    desc_info_lines  = []
+    for col in table["columns"]:
+        n = col['name']
+        tp = col['dtype'] + ","
+        isu = ""
+        if "is_unique" in col:
+            if col["is_unique"]:
+                isu = 'is unique,'        
+            if 'float' in tp or 'int' in tp:
+                isu = ""
+
+        ctn = 'contains NaN,' if col.get("contains_nan") else ''
+        expval_prefx = 'Example Values: '
+        expval = str(random.sample(col["values"], min(len(col['values']), max_example_values)))
+        
+
+        if len(col['values']) > max_example_values:
+            
+            expval = expval.rsplit("]",maxsplit=1)[0] + ", ...]"
+        
+        desc_info_lines.append(
+            f"{placeholder_val} '{n}' {tp}{isu}{ctn}{expval_prefx+expval}"
+        ) 
+
+    desc_info = "\n".join(desc_info_lines)
+
+    return f"{desc_info}\n"
+
+
+def __build_table_question(
+    tables: Union[List[ColumnsTable] | List[MarkdownTable]],
+    model_config: ModelConfig,
+):
+    f = partial(__dataframe_info_simple, model_config=model_config)
+
+    df_info_list = [f(table) for table in tables]
+    return df_info_list
+
+
+def _get_full_tables(
+    tables: Union[List[ColumnsTable] | List[MarkdownTable]],
+    model_config: ModelConfig,
+    return_text=True,
+) -> str:
+    # or use the model config to jugdes which table info be returned
+    return (
+        "".join(__build_table_question(tables, model_config))
+        if return_text
+        else __build_table_question(tables, model_config)
+    )
 
 
 def _parse_chat_message_content_parts(
@@ -164,8 +306,11 @@ def _parse_chat_message_content_parts(
     tokenizer: AnyTokenizer,
 ) -> ChatMessageParseResult:
     texts: List[str] = []
-    mm_futures: List[Awaitable[MultiModalDataDict]] = []
-    modality: Literal["image", "audio"] = "image"
+    mm_futures: List[
+        Union[Awaitable[MultiModalDataDict], MultiModalDataDict]
+    ] = []
+
+    modality: Literal["image", "audio", "tabular"] = "image"
 
     for part in parts:
         part_type = part["type"]
@@ -176,22 +321,39 @@ def _parse_chat_message_content_parts(
             modality = "image"
             if len(mm_futures) > 0:
                 raise NotImplementedError(
-                    "Multiple multimodal inputs is currently not supported.")
+                    "Multiple multimodal inputs is currently not supported."
+                )
 
             image_url = _ImageParser.validate_python(part)["image_url"]
 
             if image_url.get("detail", "auto") != "auto":
                 logger.warning(
                     "'image_url.detail' is currently not supported and "
-                    "will be ignored.")
+                    "will be ignored."
+                )
 
             image_future = async_get_and_parse_image(image_url["url"])
             mm_futures.append(image_future)
+
+        elif part_type == "table":
+            if len(mm_futures) > 0:
+                raise NotImplementedError(
+                    "Multiple 'table' input is currently not supported."
+                )
+
+            table_data: Union[List[ColumnsTable] | List[MarkdownTable]] = (
+                _TabularParser.validate_python(part)["tables"]
+            )
+
+            mm_futures.append({"table": table_data})
+            modality = "tabular"
+
         elif part_type == "audio_url":
             modality = "audio"
             if len(mm_futures) > 0:
                 raise NotImplementedError(
-                    "Multiple multimodal inputs is currently not supported.")
+                    "Multiple multimodal inputs is currently not supported."
+                )
 
             audio_url = _AudioParser.validate_python(part)["audio_url"]
             audio_future = async_get_and_parse_audio(audio_url["url"])
@@ -202,18 +364,42 @@ def _parse_chat_message_content_parts(
     text_prompt = "\n".join(texts)
 
     if mm_futures:
-        placeholder_token_str = _mm_token_str(model_config, tokenizer,
-                                              modality)
-        if placeholder_token_str is not None:
-            if placeholder_token_str in text_prompt:
-                logger.warning(
-                    "Detected multi-modal token string in the text prompt. "
-                    "Skipping prompt formatting.")
-            else:
-                text_prompt = _get_full_multimodal_text_prompt(
-                    placeholder_token_str=placeholder_token_str,
-                    text_prompt=text_prompt,
-                )
+        if modality == "tabular":
+            if inspect.isawaitable(mm_futures[0]):
+                raise ValueError("tabular table data cannot awaitable")
+
+            mm_data = mm_futures[0]
+
+            table = cast(
+                Union[List[ColumnsTable] | List[MarkdownTable]],
+                mm_data["table"],
+            )
+
+            table_info_lst = _get_full_tables(
+                table, model_config, return_text=False
+            )
+
+            text_prompt = _get_full_tarbular_text_prompt(
+                placeholder_token_str="<TABLE_CONTENT>",
+                table_infos=table_info_lst,
+                text_prompt=text_prompt,
+            )
+
+        else:
+            placeholder_token_str = _mm_token_str(
+                model_config, tokenizer, modality
+            )
+            if placeholder_token_str is not None:
+                if placeholder_token_str in text_prompt:
+                    logger.warning(
+                        "Detected multi-modal token string in the text prompt. "
+                        "Skipping prompt formatting."
+                    )
+                else:
+                    text_prompt = _get_full_multimodal_text_prompt(
+                        placeholder_token_str=placeholder_token_str,
+                        text_prompt=text_prompt,
+                    )
 
     messages = [ConversationMessage(role=role, content=text_prompt)]
 
@@ -246,13 +432,17 @@ def parse_chat_messages(
     messages: List[ChatCompletionMessageParam],
     model_config: ModelConfig,
     tokenizer: AnyTokenizer,
-) -> Tuple[List[ConversationMessage], List[Awaitable[MultiModalDataDict]]]:
+) -> Tuple[
+    List[ConversationMessage],
+    List[Union[Awaitable[MultiModalDataDict], MultiModalDataDict]],
+]:
     conversation: List[ConversationMessage] = []
-    mm_futures: List[Awaitable[MultiModalDataDict]] = []
+    mm_futures: List[
+        Union[Awaitable[MultiModalDataDict], MultiModalDataDict]
+    ] = []
 
     for msg in messages:
-        parse_result = _parse_chat_message_content(msg, model_config,
-                                                   tokenizer)
+        parse_result = _parse_chat_message_content(msg, model_config, tokenizer)
 
         conversation.extend(parse_result.messages)
         mm_futures.extend(parse_result.mm_futures)
@@ -272,7 +462,8 @@ def apply_chat_template(
         raise ValueError(
             "As of transformers v4.44, default chat template is no longer "
             "allowed, so you must provide a chat template if the tokenizer "
-            "does not define one.")
+            "does not define one."
+        )
 
     prompt = tokenizer.apply_chat_template(
         conversation=conversation,
diff --git a/vllm/entrypoints/llm.py b/vllm/entrypoints/llm.py
index 31175724..c4e6ae08 100644
--- a/vllm/entrypoints/llm.py
+++ b/vllm/entrypoints/llm.py
@@ -5,25 +5,32 @@ from tqdm import tqdm
 
 from vllm.engine.arg_utils import EngineArgs
 from vllm.engine.llm_engine import LLMEngine
-from vllm.entrypoints.chat_utils import (ChatCompletionMessageParam,
-                                         apply_chat_template,
-                                         parse_chat_messages)
+from vllm.entrypoints.chat_utils import (
+    ChatCompletionMessageParam,
+    apply_chat_template,
+    parse_chat_messages,
+)
 from vllm.inputs import PromptInputs, TextPrompt, TokensPrompt
 from vllm.inputs.parse import parse_and_batch_prompt
 from vllm.logger import init_logger
 from vllm.lora.request import LoRARequest
 from vllm.model_executor.guided_decoding import (
-    GuidedDecodingRequest, get_local_guided_decoding_logits_processor)
+    GuidedDecodingRequest,
+    get_local_guided_decoding_logits_processor,
+)
 from vllm.model_executor.guided_decoding.guided_fields import LLMGuidedOptions
 from vllm.outputs import EmbeddingRequestOutput, RequestOutput
 from vllm.pooling_params import PoolingParams
 from vllm.prompt_adapter.request import PromptAdapterRequest
 from vllm.sampling_params import SamplingParams
-from vllm.transformers_utils.tokenizer import (AnyTokenizer,
-                                               get_cached_tokenizer)
+from vllm.transformers_utils.tokenizer import AnyTokenizer, get_cached_tokenizer
 from vllm.transformers_utils.tokenizer_group import TokenizerGroup
 from vllm.usage.usage_lib import UsageContext
 from vllm.utils import Counter, deprecate_kwargs
+from vllm.inputs.data import TokensPrompt
+
+from vllm.entrypoints.openai.serving_chat import _table_tokenizer_insert
+
 
 logger = init_logger(__name__)
 
@@ -131,14 +138,14 @@ class LLM:
         disable_custom_all_reduce: bool = False,
         **kwargs,
     ) -> None:
-        '''
+        """
         LLM constructor.
 
         Note: if enforce_eager is unset (enforce_eager is None)
         it defaults to False for decoder-only models and True
         for encoder/decoder models, since encoder/decoder models
         do not currently support CUDAGraph.
-        '''
+        """
 
         if "disable_log_stats" not in kwargs:
             kwargs["disable_log_stats"] = True
@@ -150,7 +157,8 @@ class LLM:
         )
         if any(k in kwargs for k in removed_vision_keys):
             raise TypeError(
-                "There is no need to pass vision-related arguments anymore.")
+                "There is no need to pass vision-related arguments anymore."
+            )
         engine_args = EngineArgs(
             model=model,
             tokenizer=tokenizer,
@@ -173,7 +181,8 @@ class LLM:
             **kwargs,
         )
         self.llm_engine = LLMEngine.from_engine_args(
-            engine_args, usage_context=UsageContext.LLM_CLASS)
+            engine_args, usage_context=UsageContext.LLM_CLASS
+        )
         self.request_counter = Counter()
 
     def get_tokenizer(self) -> AnyTokenizer:
@@ -194,51 +203,51 @@ class LLM:
     def generate(
         self,
         prompts: str,
-        sampling_params: Optional[Union[SamplingParams,
-                                        List[SamplingParams]]] = None,
+        sampling_params: Optional[
+            Union[SamplingParams, List[SamplingParams]]
+        ] = None,
         prompt_token_ids: Optional[List[int]] = None,
         use_tqdm: bool = True,
         lora_request: Optional[Union[List[LoRARequest], LoRARequest]] = None,
-    ) -> List[RequestOutput]:
-        ...
+    ) -> List[RequestOutput]: ...
 
     @overload  # LEGACY: multi (prompt + optional token ids)
     def generate(
         self,
         prompts: List[str],
-        sampling_params: Optional[Union[SamplingParams,
-                                        List[SamplingParams]]] = None,
+        sampling_params: Optional[
+            Union[SamplingParams, List[SamplingParams]]
+        ] = None,
         prompt_token_ids: Optional[List[List[int]]] = None,
         use_tqdm: bool = True,
         lora_request: Optional[Union[List[LoRARequest], LoRARequest]] = None,
-    ) -> List[RequestOutput]:
-        ...
+    ) -> List[RequestOutput]: ...
 
     @overload  # LEGACY: single (token ids + optional prompt)
     def generate(
         self,
         prompts: Optional[str] = None,
-        sampling_params: Optional[Union[SamplingParams,
-                                        List[SamplingParams]]] = None,
+        sampling_params: Optional[
+            Union[SamplingParams, List[SamplingParams]]
+        ] = None,
         *,
         prompt_token_ids: List[int],
         use_tqdm: bool = True,
         lora_request: Optional[Union[List[LoRARequest], LoRARequest]] = None,
-    ) -> List[RequestOutput]:
-        ...
+    ) -> List[RequestOutput]: ...
 
     @overload  # LEGACY: multi (token ids + optional prompt)
     def generate(
         self,
         prompts: Optional[List[str]] = None,
-        sampling_params: Optional[Union[SamplingParams,
-                                        List[SamplingParams]]] = None,
+        sampling_params: Optional[
+            Union[SamplingParams, List[SamplingParams]]
+        ] = None,
         *,
         prompt_token_ids: List[List[int]],
         use_tqdm: bool = True,
         lora_request: Optional[Union[List[LoRARequest], LoRARequest]] = None,
-    ) -> List[RequestOutput]:
-        ...
+    ) -> List[RequestOutput]: ...
 
     @overload  # LEGACY: single or multi token ids [pos-only]
     def generate(
@@ -248,8 +257,7 @@ class LLM:
         prompt_token_ids: Union[List[int], List[List[int]]],
         use_tqdm: bool = True,
         lora_request: Optional[Union[List[LoRARequest], LoRARequest]] = None,
-    ) -> List[RequestOutput]:
-        ...
+    ) -> List[RequestOutput]: ...
 
     @overload
     def generate(
@@ -257,12 +265,12 @@ class LLM:
         inputs: Union[PromptInputs, Sequence[PromptInputs]],
         /,  # We may enable `inputs` keyword after removing the old API
         *,
-        sampling_params: Optional[Union[SamplingParams,
-                                        Sequence[SamplingParams]]] = None,
+        sampling_params: Optional[
+            Union[SamplingParams, Sequence[SamplingParams]]
+        ] = None,
         use_tqdm: bool = True,
         lora_request: Optional[Union[List[LoRARequest], LoRARequest]] = None,
-    ) -> List[RequestOutput]:
-        ...
+    ) -> List[RequestOutput]: ...
 
     @deprecate_kwargs(
         "prompts",
@@ -272,16 +280,20 @@ class LLM:
     )
     def generate(
         self,
-        prompts: Union[Union[PromptInputs, Sequence[PromptInputs]],
-                       Optional[Union[str, List[str]]]] = None,
-        sampling_params: Optional[Union[SamplingParams,
-                                        Sequence[SamplingParams]]] = None,
+        prompts: Union[
+            Union[PromptInputs, Sequence[PromptInputs]],
+            Optional[Union[str, List[str]]],
+        ] = None,
+        sampling_params: Optional[
+            Union[SamplingParams, Sequence[SamplingParams]]
+        ] = None,
         prompt_token_ids: Optional[Union[List[int], List[List[int]]]] = None,
         use_tqdm: bool = True,
         lora_request: Optional[Union[List[LoRARequest], LoRARequest]] = None,
         prompt_adapter_request: Optional[PromptAdapterRequest] = None,
-        guided_options_request: Optional[Union[LLMGuidedOptions,
-                                               GuidedDecodingRequest]] = None
+        guided_options_request: Optional[
+            Union[LLMGuidedOptions, GuidedDecodingRequest]
+        ] = None,
     ) -> List[RequestOutput]:
         """Generates the completions for the input prompts.
 
@@ -313,7 +325,8 @@ class LLM:
         if self.llm_engine.model_config.embedding_mode:
             raise ValueError(
                 "LLM.generate() is only supported for (conditional) generation "
-                "models (XForCausalLM, XForConditionalGeneration).")
+                "models (XForCausalLM, XForConditionalGeneration)."
+            )
 
         if prompt_token_ids is not None:
             inputs = self._convert_v1_inputs(
@@ -327,9 +340,11 @@ class LLM:
             if len(guided_options_request) > 1:
                 raise ValueError(
                     "You can only use one guided decoding but multiple is "
-                    f"specified: {guided_options_request}")
+                    f"specified: {guided_options_request}"
+                )
             guided_options_request = GuidedDecodingRequest(
-                **guided_options_request)
+                **guided_options_request
+            )
 
         if sampling_params is None:
             # Use default sampling params.
@@ -340,7 +355,8 @@ class LLM:
             params=sampling_params,
             lora_request=lora_request,
             prompt_adapter_request=prompt_adapter_request,
-            guided_options=guided_options_request)
+            guided_options=guided_options_request,
+        )
 
         outputs = self._run_engine(use_tqdm=use_tqdm)
         return LLMEngine.validate_outputs(outputs, RequestOutput)
@@ -348,8 +364,9 @@ class LLM:
     def chat(
         self,
         messages: List[ChatCompletionMessageParam],
-        sampling_params: Optional[Union[SamplingParams,
-                                        List[SamplingParams]]] = None,
+        sampling_params: Optional[
+            Union[SamplingParams, List[SamplingParams]]
+        ] = None,
         use_tqdm: bool = True,
         lora_request: Optional[LoRARequest] = None,
         chat_template: Optional[str] = None,
@@ -385,14 +402,135 @@ class LLM:
         tokenizer = self.get_tokenizer()
         model_config = self.llm_engine.get_model_config()
 
-        conversations, _ = parse_chat_messages(messages, model_config,
-                                               tokenizer)
+        conversations, mm_futures = parse_chat_messages(
+            messages, model_config, tokenizer
+        )
 
-        prompts = apply_chat_template(
+        prompt = apply_chat_template(
             tokenizer,
             conversations,
             chat_template=chat_template,
-            add_generation_prompt=add_generation_prompt)
+            add_generation_prompt=add_generation_prompt,
+            tokenize=False,
+        )
+        print("===============prompt token string start=================")
+        print(prompt)
+        print("===============prompt token string end===============")
+
+        if len(mm_futures):
+            # since we support only single mm data currently
+            assert len(mm_futures) == 1, (
+                "Multiple 'table' | 'image_url' | 'audio_url'"
+                "input is currently not supported."
+            )
+
+            mm_data = mm_futures[0]
+
+        if isinstance(mm_data, dict) and "table" in mm_data:
+            table_input_ids = _table_tokenizer_insert(
+                prompt, tokenizer, model_config
+            )
+
+            prompts_ids = table_input_ids
+        else:
+            prompts_ids = tokenizer(prompt, add_special_tokens=True)
+
+        prompts = TokensPrompt(
+            prompt_token_ids=prompts_ids, multi_modal_data=mm_data
+        )
+
+        return self.generate(
+            prompts,
+            sampling_params,
+            use_tqdm=use_tqdm,
+            lora_request=lora_request,
+        )
+
+    def batch_chat(
+        self,
+        messages: List[List[ChatCompletionMessageParam]],
+        sampling_params: Optional[
+            Union[SamplingParams, List[SamplingParams]]
+        ] = None,
+        use_tqdm: bool = True,
+        lora_request: Optional[LoRARequest] = None,
+        chat_template: Optional[str] = None,
+        add_generation_prompt: bool = True,
+    ) -> List[RequestOutput]:
+        """Generates chat responses for a list of messages.
+
+        This method tokenizes the messages and calls the :meth:`generate`
+        method to produce responses.
+
+        Args:
+            messages: List of messages, where each message is a list of
+                dictionaries with 'role' and 'content' keys.
+            sampling_params: Parameters for text generation; defaults apply if None.
+                Single values are applied universally, while a list must match the
+                number of prompts for one-to-one pairing.
+            use_tqdm: Flag to display a progress bar using tqdm.
+            lora_request: Optional LoRA request for customization during generation.
+            chat_template: Optional template for structuring chat; defaults to
+                model's template if not provided.
+            add_generation_prompt: If True, prepends a generation template to each message.
+
+        Returns:
+            List of ``RequestOutput`` objects with responses, ordered to match
+            the input messages.
+        """
+        msg_check = messages[0]
+        if isinstance(msg_check, dict):
+            return [
+                self.chat(
+                    messages=messages,
+                    sampling_params=sampling_params,
+                    use_tqdm=use_tqdm,
+                    lora_request=lora_request,
+                    chat_template=chat_template,
+                    add_generation_prompt=add_generation_prompt,
+                )
+            ]
+        elif isinstance(msg_check, list):
+            prompts = []
+
+            for msgs in messages:
+                tokenizer = self.get_tokenizer()
+                model_config = self.llm_engine.get_model_config()
+                conversations, mm_futures = parse_chat_messages(
+                    msgs, model_config, tokenizer
+                )
+
+                prompt = apply_chat_template(
+                    tokenizer,
+                    conversations,
+                    chat_template=chat_template,
+                    add_generation_prompt=add_generation_prompt,
+                    tokenize=False,
+                )
+
+                if len(mm_futures):
+                    # since we support only single mm data currently
+                    assert len(mm_futures) == 1, (
+                        "Multiple 'table' | 'image_url' | 'audio_url'"
+                        "input is currently not supported."
+                    )
+
+                    mm_data = mm_futures[0]
+
+                if isinstance(mm_data, dict) and "table" in mm_data:
+                    table_input_ids = _table_tokenizer_insert(
+                        prompt, tokenizer, model_config
+                    )
+
+                    prompts_ids = table_input_ids
+                else:
+                    prompts_ids = tokenizer(prompt, add_special_tokens=True)
+
+                prompts.append(
+                    TokensPrompt(
+                        prompt_token_ids=prompts_ids, multi_modal_data=mm_data
+                    )
+                )
 
         return self.generate(
             prompts,
@@ -405,51 +543,51 @@ class LLM:
     def encode(
         self,
         prompts: str,
-        pooling_params: Optional[Union[PoolingParams,
-                                       Sequence[PoolingParams]]] = None,
+        pooling_params: Optional[
+            Union[PoolingParams, Sequence[PoolingParams]]
+        ] = None,
         prompt_token_ids: Optional[List[int]] = None,
         use_tqdm: bool = True,
         lora_request: Optional[Union[List[LoRARequest], LoRARequest]] = None,
-    ) -> List[EmbeddingRequestOutput]:
-        ...
+    ) -> List[EmbeddingRequestOutput]: ...
 
     @overload  # LEGACY: multi (prompt + optional token ids)
     def encode(
         self,
         prompts: List[str],
-        pooling_params: Optional[Union[PoolingParams,
-                                       Sequence[PoolingParams]]] = None,
+        pooling_params: Optional[
+            Union[PoolingParams, Sequence[PoolingParams]]
+        ] = None,
         prompt_token_ids: Optional[List[List[int]]] = None,
         use_tqdm: bool = True,
         lora_request: Optional[Union[List[LoRARequest], LoRARequest]] = None,
-    ) -> List[EmbeddingRequestOutput]:
-        ...
+    ) -> List[EmbeddingRequestOutput]: ...
 
     @overload  # LEGACY: single (token ids + optional prompt)
     def encode(
         self,
         prompts: Optional[str] = None,
-        pooling_params: Optional[Union[PoolingParams,
-                                       Sequence[PoolingParams]]] = None,
+        pooling_params: Optional[
+            Union[PoolingParams, Sequence[PoolingParams]]
+        ] = None,
         *,
         prompt_token_ids: List[int],
         use_tqdm: bool = True,
         lora_request: Optional[Union[List[LoRARequest], LoRARequest]] = None,
-    ) -> List[EmbeddingRequestOutput]:
-        ...
+    ) -> List[EmbeddingRequestOutput]: ...
 
     @overload  # LEGACY: multi (token ids + optional prompt)
     def encode(
         self,
         prompts: Optional[List[str]] = None,
-        pooling_params: Optional[Union[PoolingParams,
-                                       Sequence[PoolingParams]]] = None,
+        pooling_params: Optional[
+            Union[PoolingParams, Sequence[PoolingParams]]
+        ] = None,
         *,
         prompt_token_ids: List[List[int]],
         use_tqdm: bool = True,
         lora_request: Optional[Union[List[LoRARequest], LoRARequest]] = None,
-    ) -> List[EmbeddingRequestOutput]:
-        ...
+    ) -> List[EmbeddingRequestOutput]: ...
 
     @overload  # LEGACY: single or multi token ids [pos-only]
     def encode(
@@ -459,8 +597,7 @@ class LLM:
         prompt_token_ids: Union[List[int], List[List[int]]],
         use_tqdm: bool = True,
         lora_request: Optional[Union[List[LoRARequest], LoRARequest]] = None,
-    ) -> List[EmbeddingRequestOutput]:
-        ...
+    ) -> List[EmbeddingRequestOutput]: ...
 
     @overload
     def encode(
@@ -468,12 +605,12 @@ class LLM:
         inputs: Union[PromptInputs, Sequence[PromptInputs]],
         /,  # We may enable `inputs` keyword after removing the old API
         *,
-        pooling_params: Optional[Union[PoolingParams,
-                                       Sequence[PoolingParams]]] = None,
+        pooling_params: Optional[
+            Union[PoolingParams, Sequence[PoolingParams]]
+        ] = None,
         use_tqdm: bool = True,
         lora_request: Optional[Union[List[LoRARequest], LoRARequest]] = None,
-    ) -> List[EmbeddingRequestOutput]:
-        ...
+    ) -> List[EmbeddingRequestOutput]: ...
 
     @deprecate_kwargs(
         "prompts",
@@ -483,10 +620,13 @@ class LLM:
     )
     def encode(
         self,
-        prompts: Union[Union[PromptInputs, Sequence[PromptInputs]],
-                       Optional[Union[str, List[str]]]] = None,
-        pooling_params: Optional[Union[PoolingParams,
-                                       Sequence[PoolingParams]]] = None,
+        prompts: Union[
+            Union[PromptInputs, Sequence[PromptInputs]],
+            Optional[Union[str, List[str]]],
+        ] = None,
+        pooling_params: Optional[
+            Union[PoolingParams, Sequence[PoolingParams]]
+        ] = None,
         prompt_token_ids: Optional[Union[List[int], List[List[int]]]] = None,
         use_tqdm: bool = True,
         lora_request: Optional[Union[List[LoRARequest], LoRARequest]] = None,
@@ -564,15 +704,19 @@ class LLM:
         if prompts is not None:
             num_requests = len(prompts)
         if prompt_token_ids is not None:
-            if (num_requests is not None
-                    and num_requests != len(prompt_token_ids)):
-                raise ValueError("The lengths of prompts and prompt_token_ids "
-                                 "must be the same.")
+            if num_requests is not None and num_requests != len(
+                prompt_token_ids
+            ):
+                raise ValueError(
+                    "The lengths of prompts and prompt_token_ids "
+                    "must be the same."
+                )
 
             num_requests = len(prompt_token_ids)
         if num_requests is None:
-            raise ValueError("Either prompts or prompt_token_ids must be "
-                             "provided.")
+            raise ValueError(
+                "Either prompts or prompt_token_ids must be " "provided."
+            )
 
         inputs: List[PromptInputs] = []
         for i in range(num_requests):
@@ -592,8 +736,12 @@ class LLM:
     def _validate_and_add_requests(
         self,
         inputs: Union[PromptInputs, Sequence[PromptInputs]],
-        params: Union[SamplingParams, Sequence[SamplingParams], PoolingParams,
-                      Sequence[PoolingParams]],
+        params: Union[
+            SamplingParams,
+            Sequence[SamplingParams],
+            PoolingParams,
+            Sequence[PoolingParams],
+        ],
         lora_request: Optional[Union[Sequence[LoRARequest], LoRARequest]],
         prompt_adapter_request: Optional[PromptAdapterRequest],
         guided_options: Optional[GuidedDecodingRequest] = None,
@@ -605,17 +753,19 @@ class LLM:
         num_requests = len(inputs)
 
         if isinstance(params, list) and len(params) != num_requests:
-            raise ValueError("The lengths of prompts and params "
-                             "must be the same.")
-        if isinstance(lora_request,
-                      list) and len(lora_request) != num_requests:
-            raise ValueError("The lengths of prompts and lora_request "
-                             "must be the same.")
+            raise ValueError(
+                "The lengths of prompts and params " "must be the same."
+            )
+        if isinstance(lora_request, list) and len(lora_request) != num_requests:
+            raise ValueError(
+                "The lengths of prompts and lora_request " "must be the same."
+            )
 
         if isinstance(params, list):
             params = [
                 self._add_guided_processor(param, guided_options)
-                if isinstance(param, SamplingParams) else param
+                if isinstance(param, SamplingParams)
+                else param
                 for param in params
             ]
         elif isinstance(params, SamplingParams):
@@ -626,8 +776,9 @@ class LLM:
             self._add_request(
                 request_inputs,
                 params[i] if isinstance(params, Sequence) else params,
-                lora_request=lora_request[i] if isinstance(
-                    lora_request, Sequence) else lora_request,
+                lora_request=lora_request[i]
+                if isinstance(lora_request, Sequence)
+                else lora_request,
                 prompt_adapter_request=prompt_adapter_request,
             )
 
@@ -648,17 +799,23 @@ class LLM:
         )
 
     def _add_guided_processor(
-            self,
-            params: SamplingParams,
-            guided_options: Optional[GuidedDecodingRequest] = None):
+        self,
+        params: SamplingParams,
+        guided_options: Optional[GuidedDecodingRequest] = None,
+    ):
         if guided_options:
             if guided_options.guided_decoding_backend is None:
                 decoding_config = self.llm_engine.get_decoding_config()
                 guided_options.guided_decoding_backend = (
-                    decoding_config.guided_decoding_backend)
-            guided_logits_processor = get_local_guided_decoding_logits_processor(  #noqa
-                guided_options.guided_decoding_backend, guided_options,
-                self.get_tokenizer())
+                    decoding_config.guided_decoding_backend
+                )
+            guided_logits_processor = (
+                get_local_guided_decoding_logits_processor(  # noqa
+                    guided_options.guided_decoding_backend,
+                    guided_options,
+                    self.get_tokenizer(),
+                )
+            )
             if guided_logits_processor:
                 if params.logits_processors is None:
                     params.logits_processors = []
@@ -666,7 +823,7 @@ class LLM:
         return params
 
     def _run_engine(
-            self, *, use_tqdm: bool
+        self, *, use_tqdm: bool
     ) -> List[Union[RequestOutput, EmbeddingRequestOutput]]:
         # Initialize tqdm.
         if use_tqdm:
@@ -675,8 +832,10 @@ class LLM:
                 total=num_requests,
                 desc="Processed prompts",
                 dynamic_ncols=True,
-                postfix=(f"est. speed input: {0:.2f} toks/s, "
-                         f"output: {0:.2f} toks/s"),
+                postfix=(
+                    f"est. speed input: {0:.2f} toks/s, "
+                    f"output: {0:.2f} toks/s"
+                ),
             )
         # Run the engine.
         outputs: List[Union[RequestOutput, EmbeddingRequestOutput]] = []
@@ -693,12 +852,15 @@ class LLM:
                             total_in_toks += len(output.prompt_token_ids)
                             in_spd = total_in_toks / pbar.format_dict["elapsed"]
                             total_out_toks += sum(
-                                len(stp.token_ids) for stp in output.outputs)
-                            out_spd = (total_out_toks /
-                                       pbar.format_dict["elapsed"])
+                                len(stp.token_ids) for stp in output.outputs
+                            )
+                            out_spd = (
+                                total_out_toks / pbar.format_dict["elapsed"]
+                            )
                             pbar.postfix = (
                                 f"est. speed input: {in_spd:.2f} toks/s, "
-                                f"output: {out_spd:.2f} toks/s")
+                                f"output: {out_spd:.2f} toks/s"
+                            )
                         pbar.update(1)
         if use_tqdm:
             pbar.close()
diff --git a/vllm/entrypoints/openai/serving_chat.py b/vllm/entrypoints/openai/serving_chat.py
index 4d8e240a..bc8bf0ba 100644
--- a/vllm/entrypoints/openai/serving_chat.py
+++ b/vllm/entrypoints/openai/serving_chat.py
@@ -1,10 +1,12 @@
 import asyncio
+import inspect
 import time
 from typing import AsyncGenerator, AsyncIterator, Dict, Final, List, Optional
 from typing import Sequence as GenericSequence
 from typing import Union
 
 from fastapi import Request
+from transformers import PreTrainedTokenizer
 
 from vllm.config import ModelConfig
 from vllm.engine.protocol import AsyncEngineClient
@@ -22,7 +24,8 @@ from vllm.entrypoints.openai.protocol import (
     FunctionCall, ToolCall, UsageInfo)
 from vllm.entrypoints.openai.serving_engine import (LoRAModulePath,
                                                     OpenAIServing,
-                                                    PromptAdapterPath)
+                                                    PromptAdapterPath,
+                                                    TextTokensPrompt)
 from vllm.inputs import TokensPrompt
 from vllm.logger import init_logger
 from vllm.multimodal import MultiModalDataDict
@@ -36,6 +39,64 @@ from vllm.utils import iterate_with_cancellation, random_uuid
 logger = init_logger(__name__)
 
 
+def _table_tokenizer_insert(prompt: str, tokenizer: PreTrainedTokenizer,
+                            model_config: ModelConfig) -> List[int]:
+    '''
+    Tokenizes the input prompt by inserting a separator token 
+    between each chunk of text.
+
+    Args:
+        prompt (str): The input prompt to be tokenized. 
+                    It contains one or more instances of the 
+                    INSERT_EMBS_TOKEN.
+        tokenizer (transformers.PreTrainedTokenizer): 
+            The tokenizer object used for tokenization.
+
+    Returns:
+       List[int]: The tokenized input prompt as a list of input IDs. 
+
+    '''
+
+    hf_config = model_config.hf_config
+
+    # get placeholder is None then get the insert embeds token, 
+    # if not has the token , should raise
+    placeholder_token = getattr(hf_config,
+                                "placeholder_token",
+                                None) or getattr(hf_config.encoder_config,
+                                                 "insert_embs_token")
+    placeholder_token_id = getattr(hf_config,
+                                "placeholder_token_id",
+                                None) or getattr(hf_config.encoder_config,
+                                                 "insert_embs_token_id")
+
+    _repeated_count = 1 if hf_config.model_type.endswith("markup") else 3
+
+    prompt_chunks = [
+        tokenizer(e,
+                  padding="longest",
+                  max_length=tokenizer.model_max_length,
+                  truncation=True).input_ids
+        for e in prompt.split(placeholder_token)
+    ]
+
+    def insert_separator(X, sep):
+        return [ele for sublist in zip(X, [sep] * len(X))
+                for ele in sublist][:-1]
+
+    input_ids = []
+    offset = 0
+    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[
+            0][0] == tokenizer.bos_token_id:
+        offset = 1
+        input_ids.append(prompt_chunks[0][0])
+
+    for x in insert_separator(prompt_chunks,
+                              [placeholder_token_id] * _repeated_count * (offset + 1)):
+        input_ids.extend(x[offset:])
+    return input_ids
+
+
 class OpenAIServingChat(OpenAIServing):
 
     def __init__(
@@ -90,6 +151,7 @@ class OpenAIServingChat(OpenAIServing):
             ) = self._maybe_get_adapters(request)
 
             model_config = self.model_config
+
             tokenizer = await self.async_engine_client.get_tokenizer(
                 lora_request)
 
@@ -117,10 +179,14 @@ class OpenAIServingChat(OpenAIServing):
         try:
             if len(mm_futures):
                 # since we support only single mm data currently
-                assert len(
-                    mm_futures
-                ) == 1, "Multiple 'image_url' input is currently not supported."
-                mm_data = await mm_futures[0]
+                assert len(mm_futures) == 1, (
+                    "Multiple 'table' | 'image_url' | 'audio_url'"
+                    "input is currently not supported.")
+                if inspect.isawaitable(mm_futures[0]):
+                    mm_data = await mm_futures[0]
+                else:
+                    mm_data = mm_futures[0]
+
         except Exception as e:
             logger.error("Error in loading multi-modal data: %s", e)
             return self.create_error_response(str(e))
@@ -130,13 +196,23 @@ class OpenAIServingChat(OpenAIServing):
             guided_decode_logits_processor = (
                 await self._guided_decode_logits_processor(request, tokenizer))
 
-            prompt_inputs = self._tokenize_prompt_input(
-                request,
-                tokenizer,
-                prompt,
-                truncate_prompt_tokens=request.truncate_prompt_tokens,
-                add_special_tokens=request.add_special_tokens,
-            )
+            prompt_inputs = None
+
+            if isinstance(mm_data, dict) and "table" in mm_data:
+                table_input_ids = _table_tokenizer_insert(
+                    prompt, tokenizer, self.model_config)
+
+                prompt_inputs = TextTokensPrompt(
+                    prompt_token_ids=table_input_ids, prompt=prompt)
+
+            else:
+                prompt_inputs = self._tokenize_prompt_input(
+                    request,
+                    tokenizer,
+                    prompt,
+                    truncate_prompt_tokens=request.truncate_prompt_tokens,
+                    add_special_tokens=request.add_special_tokens,
+                )
 
             sampling_params = request.to_sampling_params(
                 tokenizer,
diff --git a/vllm/model_executor/models/__init__.py b/vllm/model_executor/models/__init__.py
index 8591c276..322bcd5c 100644
--- a/vllm/model_executor/models/__init__.py
+++ b/vllm/model_executor/models/__init__.py
@@ -62,7 +62,7 @@ _GENERATION_MODELS = {
     "MedusaModel": ("medusa", "Medusa"),
     "EAGLEModel": ("eagle", "EAGLE"),
     "MLPSpeculatorPreTrainedModel": ("mlp_speculator", "MLPSpeculator"),
-    "JambaForCausalLM": ("jamba", "JambaForCausalLM"),
+    "JambaForCausalLM": ("jamba", "JambaForCausalLM")
 }
 
 _EMBEDDING_MODELS = {
@@ -85,6 +85,10 @@ _MULTIMODAL_MODELS = {
                                           "PaliGemmaForConditionalGeneration"),
     "Phi3VForCausalLM": ("phi3v", "Phi3VForCausalLM"),
     "UltravoxModel": ("ultravox", "UltravoxModel"),
+    "TableGPTContrastiveForCausalLM": ("tablegpt", 
+                                       "TableGPTContrastiveForCausalLM"),
+    "TableGPTMarkupForCausalLM": ("tablegpt", 
+                                  "TableGPTMarkupForCausalLM")
 }
 _CONDITIONAL_GENERATION_MODELS = {
     "BartModel": ("bart", "BartForConditionalGeneration"),
diff --git a/vllm/model_executor/models/codet5_encoder.py b/vllm/model_executor/models/codet5_encoder.py
new file mode 100644
index 00000000..dbe3aa17
--- /dev/null
+++ b/vllm/model_executor/models/codet5_encoder.py
@@ -0,0 +1,590 @@
+# coding=utf-8
+# Copyright 2023 Salesforce authors, The EleutherAI, and HuggingFace Teams. All rights reserved.
+""" PyTorch CodeT5+ 2B 6B 16B models.
+The implementation is mainly based on transformers.models.codegen.modeling_codegen by adding cross-attention
+and transformers.models.encoder_decoder.modeling_encoder_decoder.EncoderDecoderModel.
+"""
+from typing import Optional, Tuple, Union
+import torch
+import torch.utils.checkpoint
+from torch import nn
+
+from transformers.activations import ACT2FN
+from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions
+from transformers.modeling_utils import PreTrainedModel
+from transformers.utils import logging
+
+from vllm.transformers_utils.configs.tablegpt import CodeT5pModuleConfig
+
+logger = logging.get_logger(__name__)
+
+
+# Copied from transformers.models.gptj.modeling_gptj.fixed_pos_embedding
+def fixed_pos_embedding(x, seq_dim=1, seq_len=None):
+    dim = x.shape[-1]
+    if seq_len is None:
+        seq_len = x.shape[seq_dim]
+    inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2) / dim))
+    sinusoid_inp = (
+        torch.einsum("i , j -> i j", torch.arange(seq_len, dtype=torch.float), inv_freq).to(x.device).float()
+    )
+    return torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)
+
+
+# Copied from transformers.models.gptj.modeling_gptj.rotate_every_two
+def rotate_every_two(x):
+    x1 = x[:, :, :, ::2]
+    x2 = x[:, :, :, 1::2]
+    x = torch.stack((-x2, x1), dim=-1)
+    return x.flatten(-2)  # in einsum notation: rearrange(x, '... d j -> ... (d j)')
+
+
+# Copied from transformers.models.gptj.modeling_gptj.duplicate_interleave
+def duplicate_interleave(m):
+    """
+    A simple version of `torch.repeat_interleave` for duplicating a matrix while interleaving the copy.
+    """
+    dim0 = m.shape[0]
+    m = m.view(-1, 1)  # flatten the matrix
+    m = m.repeat(1, 2)  # repeat all elements into the 2nd dimension
+    m = m.view(dim0, -1)  # reshape into a matrix, interleaving the copy
+    return m
+
+
+# Copied from transformers.models.gptj.modeling_gptj.apply_rotary_pos_emb
+def apply_rotary_pos_emb(x, sincos, offset=0):
+    sin, cos = (duplicate_interleave(t)[None, offset: x.shape[1] + offset, None, :] for t in sincos)
+    # einsum notation for lambda t: repeat(t[offset:x.shape[1]+offset,:], "n d -> () n () (d j)", j=2)
+    return (x * cos) + (rotate_every_two(x) * sin)
+
+
+# Adapted from transformers.models.codegen.modeling_codegen.CodeGenAttention
+class CodeT5pAttention(nn.Module):
+    def __init__(self, config, is_cross_attention=False, is_decoder=True):
+        super().__init__()
+
+        max_positions = config.max_position_embeddings
+        self.register_buffer(
+            "causal_mask",
+            torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8)).view(
+                1, 1, max_positions, max_positions
+            ),
+        )
+
+        self.attn_dropout = nn.Dropout(config.attn_pdrop)
+        self.resid_dropout = nn.Dropout(config.resid_pdrop)
+
+        self.embed_dim = config.hidden_size
+        self.num_attention_heads = config.num_attention_heads
+        self.head_dim = self.embed_dim // self.num_attention_heads
+        if self.head_dim * self.num_attention_heads != self.embed_dim:
+            raise ValueError(
+                f"embed_dim must be divisible by num_attention_heads (got `embed_dim`: {self.embed_dim} and"
+                f" `num_attention_heads`: {self.num_attention_heads})."
+            )
+
+        self.scale_attn = torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32)).to(torch.get_default_dtype())
+        self.is_decoder = is_decoder
+        self.is_cross_attention = is_cross_attention
+        if self.is_cross_attention:
+            self.qkv_proj = nn.Linear(self.embed_dim, self.embed_dim * 2, bias=False)
+            self.q_attn = nn.Linear(self.embed_dim, self.embed_dim, bias=False)
+        else:
+            self.qkv_proj = nn.Linear(self.embed_dim, self.embed_dim * 3, bias=False)
+
+        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)
+        self.rotary_dim = None
+        if config.rotary_dim is not None:
+            self.rotary_dim = config.rotary_dim
+
+    def _split_heads(self, x, n_head, dim_head, mp_num):
+        reshaped = x.reshape(x.shape[:-1] + (n_head // mp_num, dim_head))
+        reshaped = reshaped.reshape(x.shape[:-2] + (-1,) + reshaped.shape[-1:])
+        return reshaped
+
+    def _merge_heads(self, tensor, num_attention_heads, attn_head_size):
+        """
+        Merges attn_head_size dim and num_attn_heads dim into n_ctx
+        """
+        if len(tensor.shape) == 5:
+            tensor = tensor.permute(0, 1, 3, 2, 4).contiguous()
+        elif len(tensor.shape) == 4:
+            tensor = tensor.permute(0, 2, 1, 3).contiguous()
+        else:
+            raise ValueError(f"Input tensor rank should be one of [4, 5], but is: {len(tensor.shape)}")
+        new_shape = tensor.size()[:-2] + (num_attention_heads * attn_head_size,)
+        return tensor.view(new_shape)
+
+    def _attn(
+            self,
+            query,
+            key,
+            value,
+            attention_mask=None,
+            head_mask=None,
+    ):
+        # Keep the attention weights computation in fp32 to avoid overflow issues
+        query = query.to(torch.float32)
+        key = key.to(torch.float32)
+
+        attn_weights = torch.matmul(query, key.transpose(-1, -2))
+        attn_weights = attn_weights / self.scale_attn
+
+        if not self.is_cross_attention and self.is_decoder:
+            # compute causal mask from causal mask buffer
+            query_length, key_length = query.size(-2), key.size(-2)
+            causal_mask = self.causal_mask[:, :, key_length - query_length: key_length, :key_length]
+            mask_value = torch.finfo(attn_weights.dtype).min
+            # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.
+            # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`
+            mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype).to(attn_weights.device)
+            attn_weights = torch.where(causal_mask.bool(), attn_weights, mask_value)
+
+        if attention_mask is not None:
+            # Apply the attention mask
+            attn_weights = attn_weights + attention_mask
+
+        attn_weights = nn.Softmax(dim=-1)(attn_weights)
+        attn_weights = attn_weights.to(value.dtype)
+        attn_weights = self.attn_dropout(attn_weights)
+
+        # Mask heads if we want to
+        if head_mask is not None:
+            attn_weights = attn_weights * head_mask
+
+        attn_output = torch.matmul(attn_weights, value)
+
+        return attn_output, attn_weights
+
+    def forward(
+            self,
+            hidden_states: Optional[torch.FloatTensor],
+            attention_mask: Optional[torch.FloatTensor] = None,
+            layer_past: Optional[Tuple[torch.Tensor]] = None,
+            head_mask: Optional[torch.FloatTensor] = None,
+            encoder_hidden_states: Optional[torch.Tensor] = None,
+            encoder_attention_mask: Optional[torch.FloatTensor] = None,
+            use_cache: Optional[bool] = False,
+            output_attentions: Optional[bool] = False,
+    ) -> Union[
+        Tuple[torch.Tensor, Tuple[torch.Tensor]],
+        Optional[Tuple[torch.Tensor, Tuple[torch.Tensor], Tuple[torch.Tensor, ...]]],
+    ]:
+
+        if encoder_hidden_states is not None:
+            if not hasattr(self, "q_attn"):
+                raise ValueError(
+                    "If class is used as cross attention, the weights `q_attn` have to be defined. "
+                    "Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`."
+                )
+
+            mp_num = 4
+            local_dim = self.head_dim * self.num_attention_heads // mp_num
+            q = self.q_attn(hidden_states)
+            q_split = q.reshape(q.shape[:-1] + (mp_num, -1))
+            query = torch.split(q_split, local_dim, dim=-1)[0]
+
+            qkv = self.qkv_proj(encoder_hidden_states)
+            qkv_split = qkv.reshape(qkv.shape[:-1] + (mp_num, -1))
+            value, key = torch.split(qkv_split, local_dim, dim=-1)
+
+            attention_mask = encoder_attention_mask
+        else:
+            qkv = self.qkv_proj(hidden_states)
+            mp_num = 4
+            qkv_split = qkv.reshape(qkv.shape[:-1] + (mp_num, -1))
+
+            local_dim = self.head_dim * self.num_attention_heads // mp_num
+            query, value, key = torch.split(qkv_split, local_dim, dim=-1)
+
+        query = self._split_heads(query, self.num_attention_heads, self.head_dim, mp_num=mp_num)
+        key = self._split_heads(key, self.num_attention_heads, self.head_dim, mp_num=mp_num)
+
+        value = self._split_heads(value, self.num_attention_heads, self.head_dim, mp_num=mp_num)
+        value = value.permute(0, 2, 1, 3)
+
+        seq_len = key.shape[1]
+        offset = 0
+
+        if layer_past is not None:
+            offset = layer_past[0].shape[-2]
+            seq_len += offset
+
+        if self.rotary_dim is not None:
+            k_rot = key[:, :, :, : self.rotary_dim]
+            k_pass = key[:, :, :, self.rotary_dim:]
+
+            q_rot = query[:, :, :, : self.rotary_dim]
+            q_pass = query[:, :, :, self.rotary_dim:]
+
+            sincos = fixed_pos_embedding(k_rot, 1, seq_len=seq_len)
+            k_rot = apply_rotary_pos_emb(k_rot, sincos, offset=offset)
+            seq_len_q = query.shape[1]
+            sincos_q = fixed_pos_embedding(q_rot, 1, seq_len=seq_len_q)
+            q_rot = apply_rotary_pos_emb(q_rot, sincos_q, offset=offset)
+
+            key = torch.cat([k_rot, k_pass], dim=-1)
+            query = torch.cat([q_rot, q_pass], dim=-1)
+        else:
+            sincos = fixed_pos_embedding(key, 1, seq_len=seq_len)
+            key = apply_rotary_pos_emb(key, sincos, offset=offset)
+            query = apply_rotary_pos_emb(query, sincos, offset=offset)
+
+        key = key.permute(0, 2, 1, 3)
+        query = query.permute(0, 2, 1, 3)
+
+        if layer_past is not None:
+            past_key = layer_past[0]
+            past_value = layer_past[1]
+            key = torch.cat((past_key, key), dim=-2)
+            value = torch.cat((past_value, value), dim=-2)
+
+        if use_cache is True:
+            present = (key, value)
+        else:
+            present = None
+
+        # compute self-attention: V x Softmax(QK^T)
+        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
+
+        attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_dim)
+        attn_output = self.out_proj(attn_output)
+        attn_output = self.resid_dropout(attn_output)
+
+        outputs = (attn_output, present)
+        if output_attentions:
+            outputs += (attn_weights,)
+
+        return outputs  # a, present, (attentions)
+
+
+# Adapted from transformers.models.codegen.modeling_codegen.CodeGenMLP
+class CodeT5pMLP(nn.Module):
+    def __init__(self, intermediate_size, config):  # in MLP: intermediate_size= 4 * embed_dim
+        super().__init__()
+        embed_dim = config.n_embd
+
+        self.fc_in = nn.Linear(embed_dim, intermediate_size)
+        self.fc_out = nn.Linear(intermediate_size, embed_dim)
+
+        self.act = ACT2FN[config.activation_function]
+        self.dropout = nn.Dropout(config.resid_pdrop)
+
+    def forward(self, hidden_states: Optional[torch.FloatTensor]) -> torch.FloatTensor:
+        hidden_states = self.fc_in(hidden_states)
+        hidden_states = self.act(hidden_states)
+        hidden_states = self.fc_out(hidden_states)
+        hidden_states = self.dropout(hidden_states)
+        return hidden_states
+
+
+# Adapted from transformers.models.codegen.modeling_codegen.CodeGenBlock
+class CodeT5pBlock(nn.Module):
+    def __init__(self, config, layer_idx=None):
+        super().__init__()
+        inner_dim = config.n_inner if config.n_inner is not None else 4 * config.n_embd
+        self.ln_1 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)
+
+        if config.is_decoder is False:
+            self.attn = CodeT5pAttention(config, is_cross_attention=False, is_decoder=False)
+        else:
+            self.attn = CodeT5pAttention(config)
+        self.mlp = CodeT5pMLP(inner_dim, config)
+
+        # Adding 1 cross-attention layer at the final decoder layer
+        self.add_cross_attention_by_layer = True \
+            if config.add_cross_attention and layer_idx == config.n_layer - 1 else False
+
+        if config.add_cross_attention and self.add_cross_attention_by_layer:
+            self.crossattention = CodeT5pAttention(config, is_cross_attention=True)
+
+    def forward(
+            self,
+            hidden_states: Optional[torch.FloatTensor],
+            layer_past: Optional[Tuple[torch.Tensor]] = None,
+            attention_mask: Optional[torch.FloatTensor] = None,
+            head_mask: Optional[torch.FloatTensor] = None,
+            encoder_hidden_states: Optional[torch.Tensor] = None,
+            encoder_attention_mask: Optional[torch.FloatTensor] = None,
+            use_cache: Optional[bool] = False,
+            output_attentions: Optional[bool] = False,
+    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:
+        residual = hidden_states
+        hidden_states = self.ln_1(hidden_states)
+        attn_outputs = self.attn(
+            hidden_states,
+            layer_past=layer_past,
+            attention_mask=attention_mask,
+            head_mask=head_mask,
+            use_cache=use_cache,
+            output_attentions=output_attentions,
+        )
+        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)
+        outputs = attn_outputs[1:]
+        feed_forward_hidden_states = self.mlp(hidden_states)
+
+        if encoder_hidden_states is not None and self.add_cross_attention_by_layer:
+            # add one self-attention block for cross-attention
+            if not hasattr(self, "crossattention"):
+                raise ValueError(
+                    f"If `encoder_hidden_states` are passed, {self} has to be instantiated with "
+                    "cross-attention layers by setting `config.add_cross_attention=True`"
+                )
+            # residual = hidden_states
+            # hidden_states = self.ln_cross_attn(residual)
+            cross_attn_outputs = self.crossattention(
+                hidden_states,
+                attention_mask=attention_mask,
+                head_mask=head_mask,
+                encoder_hidden_states=encoder_hidden_states,
+                encoder_attention_mask=encoder_attention_mask,
+                output_attentions=output_attentions,
+            )
+            xattn_output = cross_attn_outputs[0]
+            attn_output = attn_output + xattn_output
+            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights
+
+        hidden_states = attn_output + feed_forward_hidden_states + residual
+
+        if use_cache:
+            outputs = (hidden_states,) + outputs
+        else:
+            outputs = (hidden_states,) + outputs[1:]
+
+        return outputs  # hidden_states, present, (attentions)
+
+
+# Adapted from transformers.models.codegen.modeling_codegen.CodeGenPreTrainedModel
+class CodeT5pPreTrainedModel(PreTrainedModel):
+    """
+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
+    models.
+    """
+    config_class = CodeT5pModuleConfig
+    # base_model_prefix = "transformer"
+    supports_gradient_checkpointing = True
+    _no_split_modules = ["CodeT5pBlock"]
+
+    def __init__(self, *inputs, **kwargs):
+        super().__init__(*inputs, **kwargs)
+
+    def _init_weights(self, module):
+        """Initialize the weights."""
+        if isinstance(module, (nn.Linear,)):
+            # Slightly different from Mesh Transformer JAX which uses truncated_normal for initialization
+            # cf https://github.com/pytorch/pytorch/pull/5617
+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
+            if module.bias is not None:
+                module.bias.data.zero_()
+        elif isinstance(module, nn.Embedding):
+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
+            if module.padding_idx is not None:
+                module.weight.data[module.padding_idx].zero_()
+        elif isinstance(module, nn.LayerNorm):
+            module.bias.data.zero_()
+            module.weight.data.fill_(1.0)
+
+    def _set_gradient_checkpointing(self, module, value=False):
+        if isinstance(module, CodeT5pModel):
+            module.gradient_checkpointing = value
+
+
+# Adapted from transformers.models.codegen.modeling_codegen.CodeGenModel
+class CodeT5pModel(CodeT5pPreTrainedModel):
+    def __init__(self, config):
+        super().__init__(config)
+
+        self.embed_dim = config.n_embd
+        self.vocab_size = config.vocab_size
+        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)
+        self.drop = nn.Dropout(config.embd_pdrop)
+        self.h = nn.ModuleList([CodeT5pBlock(config, idx) for idx in range(config.n_layer)])
+        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)
+        self.rotary_dim = min(config.rotary_dim, config.n_ctx // config.num_attention_heads)
+
+        self.gradient_checkpointing = False
+
+        # Initialize weights and apply final processing
+        self.post_init()
+
+    def get_input_embeddings(self):
+        return self.wte
+
+    def set_input_embeddings(self, new_embeddings):
+        self.wte = new_embeddings
+
+    def forward(
+            self,
+            input_ids: Optional[torch.LongTensor] = None,
+            past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
+            attention_mask: Optional[torch.FloatTensor] = None,
+            token_type_ids: Optional[torch.LongTensor] = None,
+            position_ids: Optional[torch.LongTensor] = None,
+            head_mask: Optional[torch.FloatTensor] = None,
+            inputs_embeds: Optional[torch.FloatTensor] = None,
+            encoder_hidden_states: Optional[torch.Tensor] = None,
+            encoder_attention_mask: Optional[torch.FloatTensor] = None,
+            use_cache: Optional[bool] = None,
+            output_attentions: Optional[bool] = None,
+            output_hidden_states: Optional[bool] = None,
+            return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        if input_ids is not None and inputs_embeds is not None:
+            raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
+        elif input_ids is not None:
+            input_shape = input_ids.size()
+            input_ids = input_ids.view(-1, input_shape[-1])
+            batch_size = input_ids.shape[0]
+        elif inputs_embeds is not None:
+            input_shape = inputs_embeds.size()[:-1]
+            batch_size = inputs_embeds.shape[0]
+        else:
+            raise ValueError("You have to specify either input_ids or inputs_embeds")
+
+        device = input_ids.device if input_ids is not None else inputs_embeds.device
+
+        if token_type_ids is not None:
+            token_type_ids = token_type_ids.view(-1, input_shape[-1])
+
+        if position_ids is not None:
+            position_ids = position_ids.view(-1, input_shape[-1])
+
+        if past_key_values is None:
+            past_length = 0
+            past_key_values = tuple([None] * len(self.h))
+        else:
+            past_length = past_key_values[0][0].size(-2)
+
+        if position_ids is None:
+            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)
+            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
+
+        # Attention mask.
+        if attention_mask is not None:
+            if batch_size <= 0:
+                raise ValueError("batch_size has to be defined and > 0")
+            attention_mask = attention_mask.view(batch_size, -1)
+            # We create a 3D attention mask from a 2D tensor mask.
+            # Sizes are [batch_size, 1, 1, to_seq_length]
+            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]
+            # this attention mask is more simple than the triangular masking of causal attention
+            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.
+            attention_mask = attention_mask[:, None, None, :]
+
+            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
+            # masked positions, this operation will create a tensor which is 0.0 for
+            # positions we want to attend and the dtype's smallest value for masked positions.
+            # Since we are adding it to the raw scores before the softmax, this is
+            # effectively the same as removing these entirely.
+            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility
+            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min
+
+        # If a 2D or 3D attention mask is provided for the cross-attention
+        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]
+        if self.config.add_cross_attention and encoder_hidden_states is not None:
+            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()
+            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)
+            if encoder_attention_mask is None:
+                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)
+            encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)
+        else:
+            encoder_attention_mask = None
+
+        # Prepare head mask if needed
+        # 1.0 in head_mask indicate we keep the head
+        # attention_probs has shape bsz x num_attention_heads x N x N
+        # head_mask has shape n_layer x batch x num_attention_heads x N x N
+        head_mask = self.get_head_mask(head_mask, self.config.n_layer)
+
+        if inputs_embeds is None:
+            inputs_embeds = self.wte(input_ids)
+
+        hidden_states = inputs_embeds
+
+        if token_type_ids is not None:
+            token_type_embeds = self.wte(token_type_ids)
+            hidden_states = hidden_states + token_type_embeds
+
+        hidden_states = self.drop(hidden_states)
+
+        output_shape = input_shape + (hidden_states.size(-1),)
+
+        presents = () if use_cache else None
+        all_self_attentions = () if output_attentions else None
+        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None
+        all_hidden_states = () if output_hidden_states else None
+        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):
+            if output_hidden_states:
+                all_hidden_states = all_hidden_states + (hidden_states,)
+
+            if self.gradient_checkpointing and self.training:
+                if use_cache:
+                    logger.warning(
+                        "`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting "
+                        "`use_cache=False`..."
+                    )
+                    use_cache = False
+
+                def create_custom_forward(module):
+                    def custom_forward(*inputs):
+                        # None for past_key_value
+                        return module(*inputs, use_cache, output_attentions)
+
+                    return custom_forward
+
+                outputs = torch.utils.checkpoint.checkpoint(
+                    create_custom_forward(block),
+                    hidden_states,
+                    None,
+                    attention_mask,
+                    head_mask[i],
+                    encoder_hidden_states,
+                    encoder_attention_mask,
+                )
+            else:
+                outputs = block(
+                    hidden_states,
+                    layer_past=layer_past,
+                    attention_mask=attention_mask,
+                    head_mask=head_mask[i],
+                    encoder_hidden_states=encoder_hidden_states,
+                    encoder_attention_mask=encoder_attention_mask,
+                    use_cache=use_cache,
+                    output_attentions=output_attentions,
+                )
+
+            hidden_states = outputs[0]
+            if use_cache is True:
+                presents = presents + (outputs[1],)
+
+            if output_attentions:
+                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)
+                if self.config.add_cross_attention and self.add_cross_attention_by_layer:
+                    all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)
+
+        hidden_states = self.ln_f(hidden_states)
+
+        hidden_states = hidden_states.view(output_shape)
+        # Add last hidden state
+        if output_hidden_states:
+            all_hidden_states = all_hidden_states + (hidden_states,)
+
+        if not return_dict:
+            return tuple(
+                v for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions] if
+                v is not None)
+
+        return BaseModelOutputWithPastAndCrossAttentions(
+            last_hidden_state=hidden_states,
+            past_key_values=presents,
+            hidden_states=all_hidden_states,
+            attentions=all_self_attentions,
+            cross_attentions=all_cross_attentions,
+        )
\ No newline at end of file
diff --git a/vllm/model_executor/models/qwen2.py b/vllm/model_executor/models/qwen2.py
index b95987c1..3a34a7c7 100644
--- a/vllm/model_executor/models/qwen2.py
+++ b/vllm/model_executor/models/qwen2.py
@@ -356,10 +356,15 @@ class Qwen2ForCausalLM(nn.Module, SupportsLoRA):
         positions: torch.Tensor,
         kv_caches: List[torch.Tensor],
         attn_metadata: AttentionMetadata,
+        inputs_embeds: Optional[IntermediateTensors] = None,
         intermediate_tensors: Optional[IntermediateTensors] = None,
     ) -> torch.Tensor:
-        hidden_states = self.model(input_ids, positions, kv_caches,
-                                   attn_metadata, intermediate_tensors)
+        hidden_states = self.model(input_ids,
+                                   positions,
+                                   kv_caches,
+                                   attn_metadata,
+                                   intermediate_tensors,
+                                   inputs_embeds=inputs_embeds)
         return hidden_states
 
     def compute_logits(
diff --git a/vllm/model_executor/models/tablegpt.py b/vllm/model_executor/models/tablegpt.py
new file mode 100644
index 00000000..3e326634
--- /dev/null
+++ b/vllm/model_executor/models/tablegpt.py
@@ -0,0 +1,434 @@
+# coding=utf-8
+# Adapted from
+from array import array
+import random
+import itertools
+import string
+from typing import Iterable, List, Optional, Tuple,Mapping,Dict
+
+import torch
+from torch import nn
+
+from vllm.attention import AttentionMetadata
+from vllm.config import CacheConfig, LoRAConfig, MultiModalConfig, ModelConfig
+from vllm.inputs import INPUT_REGISTRY, InputContext, LLMInputs
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.multimodal import MULTIMODAL_REGISTRY
+from vllm.multimodal.utils import cached_get_tokenizer
+
+from vllm.sequence import IntermediateTensors, SamplerOutput
+from vllm.sequence import VLLM_TOKEN_ID_ARRAY_TYPE, SequenceData
+from vllm.transformers_utils.configs import TableGPTMarkUpConfig,TableGPTContrastiveConfig
+from vllm.multimodal.base import MultiModalInputs
+
+from .interfaces import SupportsMultiModal
+from .tablegpt_encoder import input_processor_for_tablegpt_encoder, load_encoder, dummy_data_for_contrastive_tablegpt
+from .utils import filter_weights, init_vllm_registered_model,merge_multimodal_embeddings
+
+from vllm.model_executor.models.codet5_encoder import CodeT5pModel
+
+_TABLE_TOKEN_COUNT_PER_COL = 3
+
+def input_processor_for_contrastive_table(ctx: InputContext, llm_inputs: LLMInputs):
+    multi_modal_data = llm_inputs.get("multi_modal_data")
+    if multi_modal_data is None or "table" not in multi_modal_data:
+        return llm_inputs
+
+    return input_processor_for_tablegpt_encoder(ctx, llm_inputs)
+
+# TODO: shold add the max table counts for the encoder?
+def get_max_contrastive_table_tokens(ctx: InputContext):
+
+    table_col_max_length = getattr(
+                    ctx.model_config.hf_config.encoder_config,
+                    "max_cols",
+                    100)
+    
+    return table_col_max_length * _TABLE_TOKEN_COUNT_PER_COL
+
+    
+
+@MULTIMODAL_REGISTRY.register_table_input_mapper()
+@MULTIMODAL_REGISTRY.register_max_multimodal_tokens("table",get_max_contrastive_table_tokens)
+@INPUT_REGISTRY.register_dummy_data(dummy_data_for_contrastive_tablegpt)
+@INPUT_REGISTRY.register_input_processor(input_processor_for_contrastive_table)
+class TableGPTContrastiveForCausalLM(nn.Module, SupportsMultiModal):
+
+    def __init__(
+        self,
+        config: TableGPTContrastiveConfig,
+        multimodal_config: MultiModalConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        lora_config: Optional[LoRAConfig] = None,
+    ) -> None:
+        super().__init__()
+
+        self.config = config
+        self.multimodal_config = multimodal_config
+        self.lora_config = lora_config
+
+        self.quant_config = quant_config
+
+        self.language_model = init_vllm_registered_model(
+            config.text_config, cache_config, quant_config)
+
+        if not self.config.encoder_config:
+            raise ValueError(
+                "table encoder configs cannot found in hf config.")
+
+        mlp_depth = self.config.projector_config.mlp_depth
+        encoder_hidden_size =  self.config.projector_config.encoder_hidden_size
+        decoder_hidden_size = self.config.projector_config.decoder_hidden_size
+        
+        num_heads = self.config.projector_config.num_heads
+        
+        if not self.config.projector_config.multihead:
+            num_heads = 1
+        
+        modules = [
+            nn.Linear(encoder_hidden_size, decoder_hidden_size * num_heads)
+        ]
+        for _ in range(1, mlp_depth):
+            modules.append(nn.GELU())
+            modules.append(
+                nn.Linear(decoder_hidden_size * num_heads,
+                            encoder_hidden_size * num_heads))
+        
+        self.projector = nn.Sequential(*modules)
+
+        self.encoder = load_encoder(self.config)
+
+    def _validate_get_table(self, **kwargs) -> torch.Tensor | None:
+
+        table = kwargs.pop("table", None)
+        if table is None or self.projector is None:
+            return None
+
+        return table
+
+    def forward(self,
+                input_ids: torch.Tensor,
+                positions: torch.Tensor,
+                kv_caches: List[torch.Tensor],
+                attn_metadata: AttentionMetadata,
+                intermediate_tensors: Optional[IntermediateTensors] = None,
+                **kwargs: object) -> torch.Tensor:
+        table_embeds = self._validate_get_table(**kwargs)
+
+        if table_embeds is not None:
+            
+            cur_table_embeds = self.projector(table_embeds)
+
+            cur_input_embeds = self.language_model.model.get_input_embeddings(input_ids.clamp(min=0))
+
+            inputs_embeds = merge_multimodal_embeddings(
+                input_ids, cur_input_embeds,cur_table_embeds, self.config.encoder_config.insert_embs_token_id
+            )
+
+            input_ids = None
+            
+            del table_embeds, cur_table_embeds, cur_input_embeds
+
+        else:
+            inputs_embeds = None
+
+        hidden_states = self.language_model(
+            input_ids,
+            positions,
+            kv_caches,
+            attn_metadata,
+            inputs_embeds=inputs_embeds,
+            intermediate_tensors=intermediate_tensors)
+        return hidden_states
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[torch.Tensor]:
+        logits = self.language_model.logits_processor(
+            self.language_model.lm_head, hidden_states, sampling_metadata)
+        return logits
+
+    def sample(
+        self,
+        logits: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[SamplerOutput]:
+        next_tokens = self.language_model.sampler(logits, sampling_metadata)
+        return next_tokens
+
+    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
+        # prepare weight iterators for components
+        enc_weights, proj_weights, llm_weights = itertools.tee(weights, 3)
+
+        # load table encoder
+        enc_weights = filter_weights(enc_weights, "encoder")
+        enc_params_dict = dict(self.encoder.named_parameters())
+        for name, loaded_weight in enc_weights:
+            param = enc_params_dict[name]
+            weight_loader = getattr(param, "weight_loader",
+                                    default_weight_loader)
+            weight_loader(param, loaded_weight)
+
+        # load projector
+        proj_weights = filter_weights(proj_weights, "projector")
+        proj_params_dict = dict(self.projector.named_parameters())
+        for name, loaded_weight in proj_weights:
+            # should remove the model. prefix
+            name = name.replace("model.","")
+
+            param = proj_params_dict[name]
+            weight_loader = getattr(param, "weight_loader",
+                                    default_weight_loader)
+            weight_loader(param, loaded_weight)
+
+        # load llm model
+        llm_weights = filter_weights(llm_weights, "decoder")
+        self.language_model.load_weights(llm_weights)
+
+
+def input_processor_for_markup_table(ctx: InputContext, llm_inputs: LLMInputs):
+    multi_modal_data = llm_inputs.get("multi_modal_data")
+    if multi_modal_data is None or "table" not in multi_modal_data:
+        return llm_inputs
+
+    placeholder_token_id = ctx.model_config.hf_config.placeholder_token_id
+    max_length = ctx.model_config.hf_config.encoder_max_length
+
+    tokenizer = cached_get_tokenizer(ctx.model_config.model,
+                                     subfolder=ctx.model_config.hf_config.encoder_config.subfolder)
+    
+    prompt_token_ids = llm_inputs["prompt_token_ids"]
+    # # NOTE: only add to the head
+    # prompt_token_ids = [placeholder_token_id] * max_length + prompt_token_ids
+    
+    # NOTE: add special id near the user query
+    prompt_token_ids = torch.tensor(prompt_token_ids)
+
+    placeholder_token_id = ctx.model_config.hf_config.placeholder_token_id
+    max_length = ctx.model_config.hf_config.encoder_max_length
+
+    indices = torch.where(prompt_token_ids == placeholder_token_id)[0]
+
+    new_prompt_token_ids = None
+
+    table_encoder_token_ids = tokenizer(multi_modal_data["table"], return_tensors="pt", truncation=True, max_length=max_length).input_ids
+    
+    if len(indices) > 0:
+        new_values = torch.tensor([placeholder_token_id] * table_encoder_token_ids.shape[-1])
+        new_prompt_token_ids = torch.cat((prompt_token_ids[:indices[0]], new_values, prompt_token_ids[indices[0] + 1:]))
+        
+    return  LLMInputs(
+        prompt_token_ids=new_prompt_token_ids.tolist() if new_prompt_token_ids is not None else prompt_token_ids.tolist(),
+        multi_modal_data={"table":table_encoder_token_ids}
+    )
+
+
+def get_max_markup_table_tokens(ctx: InputContext) -> int:
+  
+    return ctx.model_config.hf_config.encoder_max_length
+
+def dummy_tabledata_for_markup_tablegpt(
+    model_config: ModelConfig
+) -> Dict:
+    
+    import pandas as pd
+
+    def generate_random_text(length:int) -> str:
+        """random string by length"""
+        return ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(length))
+
+    encoder_max_lenth = model_config.hf_config.encoder_max_length
+
+    # df with random cell string,length:20, cols count:100, row count: 50
+    df = pd.DataFrame({
+        f'col_{i}': [generate_random_text(20) for _ in range(50)]
+        for i in range(100)
+    })
+
+
+    tokenizer = cached_get_tokenizer(model_config.model,
+                                     subfolder=model_config.hf_config.encoder_config.subfolder)
+
+    table_encoder_token_ids:torch.Tensor = tokenizer(df.to_markdown(), return_tensors="pt", truncation=True, max_length=encoder_max_lenth).input_ids
+    return {"table": table_encoder_token_ids}
+
+
+def dummy_seq_data_for_markup_tablegpt(
+    hf_config: TableGPTMarkUpConfig,
+    seq_len: int
+):  
+    
+    placeholder_token_id = hf_config.placeholder_token_id
+    encoder_max_length = hf_config.encoder_max_length
+
+    # this is the table placeholder tokens for contrastive(lly) table encoder
+    token_ids = array(VLLM_TOKEN_ID_ARRAY_TYPE,
+                      [placeholder_token_id] *  encoder_max_length)
+    
+    # extend the token ids to max seq len
+    token_ids += array(VLLM_TOKEN_ID_ARRAY_TYPE,
+                       [0]) * (seq_len - len(token_ids))
+    
+    return SequenceData(token_ids)
+
+
+
+def dummy_data_for_markup_tablegpt(ctx: InputContext, seq_len: int,
+                         mm_counts: Mapping[str, int]):
+    
+
+    # num_tables = mm_counts["table"]
+    hf_config = ctx.model_config.hf_config
+    
+    seq_data = dummy_seq_data_for_markup_tablegpt(hf_config,ctx.model_config.max_model_len)
+    
+    mm_data = dummy_tabledata_for_markup_tablegpt(ctx.model_config)
+
+    return seq_data, mm_data
+
+def markup_table_input_mapper(ctx: InputContext, data: object):
+    
+    if isinstance(data,torch.Tensor):
+        return MultiModalInputs({"table": data.to(dtype=torch.int)})
+    
+    raise ValueError("mm data must be a torch tensor")
+
+
+@MULTIMODAL_REGISTRY.register_input_mapper("table", markup_table_input_mapper)
+@MULTIMODAL_REGISTRY.register_max_multimodal_tokens("table",get_max_markup_table_tokens)
+@INPUT_REGISTRY.register_dummy_data(dummy_data_for_markup_tablegpt)
+@INPUT_REGISTRY.register_input_processor(input_processor_for_markup_table)
+class TableGPTMarkupForCausalLM(nn.Module, SupportsMultiModal):
+
+    def __init__(
+        self,
+        config: TableGPTMarkUpConfig,
+        multimodal_config: MultiModalConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        lora_config: Optional[LoRAConfig] = None,
+    ) -> None:
+        super().__init__()
+
+        self.config = config
+        self.multimodal_config = multimodal_config
+        self.lora_config = lora_config
+
+        self.quant_config = quant_config
+
+        self.language_model = init_vllm_registered_model(
+            config.text_config, cache_config, quant_config)
+
+        if not self.config.encoder_config:
+            raise ValueError(
+                "table encoder configs cannot found in hf config.")
+        
+
+        self.encoder = CodeT5pModel(self.config.encoder_config)
+
+        encoder_hidden_size=self.config.encoder_hidden_size
+        decoder_hidden_size=self.config.decoder_hidden_size
+        
+        modules = [nn.Linear(encoder_hidden_size, decoder_hidden_size)]
+        
+        for _ in range(1, self.config.mlp_depth):
+            
+            modules.append(nn.GELU())
+            
+            modules.append(nn.Linear(decoder_hidden_size, decoder_hidden_size))
+        
+        self.projector = nn.Sequential(*modules)
+
+    def _validate_get_table(self, **kwargs) -> torch.Tensor | None:
+
+        table = kwargs.pop("table", None)
+        if table is None or self.projector is None:
+            return None
+
+        return table
+
+    def forward(self,
+                input_ids: torch.Tensor,
+                positions: torch.Tensor,
+                kv_caches: List[torch.Tensor],
+                attn_metadata: AttentionMetadata,
+                intermediate_tensors: Optional[IntermediateTensors] = None,
+                **kwargs: object) -> torch.Tensor:
+        table_encoder_input_ids = self._validate_get_table(**kwargs)
+
+        if table_encoder_input_ids is not None:
+            
+            table_embeds = self.encoder(input_ids=table_encoder_input_ids).last_hidden_state
+            cur_table_embeds = self.projector(table_embeds)
+
+            cur_input_embeds = self.language_model.model.get_input_embeddings(input_ids.clamp(min=0))
+            
+            inputs_embeds = merge_multimodal_embeddings(
+                input_ids,cur_input_embeds,cur_table_embeds,self.config.placeholder_token_id
+            )
+
+            del table_embeds
+
+            input_ids = None
+
+        else:
+            inputs_embeds = None
+
+        hidden_states = self.language_model.model(
+            input_ids,
+            positions,
+            kv_caches,
+            attn_metadata,
+            inputs_embeds=inputs_embeds,
+            intermediate_tensors=intermediate_tensors)
+        return hidden_states
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[torch.Tensor]:
+        logits = self.language_model.compute_logits(hidden_states, sampling_metadata)
+        return logits
+
+    def sample(
+        self,
+        logits: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[SamplerOutput]:
+        next_tokens = self.language_model.sampler(logits, sampling_metadata)
+        return next_tokens
+
+    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
+        # prepare weight iterators for components
+        enc_weights, proj_weights, llm_weights = itertools.tee(weights, 3)
+        
+        # load llm model
+        llm_weights = filter_weights(llm_weights, "decoder")
+        self.language_model.load_weights(llm_weights)
+
+        proj_weights = filter_weights(proj_weights, "projector")
+        proj_params_dict = dict(self.projector.named_parameters())
+        for name, loaded_weight in proj_weights:
+            param = proj_params_dict[name]
+            weight_loader = getattr(param, "weight_loader",
+                                    default_weight_loader)
+            weight_loader(param, loaded_weight)
+
+        enc_weights = filter_weights(enc_weights, "encoder")
+        enc_params_dict = dict(self.encoder.named_parameters())
+
+        for name, loaded_weight in enc_weights:
+
+            if name.endswith(".causal_mask"):
+                continue
+
+            param = enc_params_dict[name]
+            weight_loader = getattr(param, "weight_loader",
+                                    default_weight_loader)
+            weight_loader(param, loaded_weight)
\ No newline at end of file
diff --git a/vllm/model_executor/models/tablegpt_encoder.py b/vllm/model_executor/models/tablegpt_encoder.py
new file mode 100644
index 00000000..e65891b3
--- /dev/null
+++ b/vllm/model_executor/models/tablegpt_encoder.py
@@ -0,0 +1,798 @@
+import string
+import random
+import typing as t
+from array import array
+
+import numpy as np
+import pandas as pd
+
+import torch
+from einops import rearrange
+from torch import einsum, nn
+from torch.nn import functional as F
+from transformers import (
+    AutoModel,
+    BertConfig,
+    PretrainedConfig,
+    PreTrainedTokenizer,
+)
+
+from vllm.config import ModelConfig
+from vllm.transformers_utils.configs import TableGPTContrastiveConfig
+from vllm.entrypoints.chat_utils import ColumnsTable
+from vllm.inputs import InputContext, LLMInputs
+from vllm.multimodal.utils import cached_get_tokenizer
+from vllm.sequence import VLLM_TOKEN_ID_ARRAY_TYPE, SequenceData
+
+TABGPT_ENCODER = None
+
+
+def load_encoder(config: PretrainedConfig):
+    global TABGPT_ENCODER
+    TABGPT_ENCODER = TableEncoder(config)
+    return TABGPT_ENCODER
+
+
+def get_embedded_table(
+    table: ColumnsTable,
+    model_config: ModelConfig,
+    tokenizer: PreTrainedTokenizer,
+):
+    table_max_rows = model_config.hf_config.encoder_config.max_rows
+    table_max_cols = model_config.hf_config.encoder_config.max_cols
+
+    df_col_count = len(table["columns"])
+
+    tb = np.array(
+        [tb_col["values"][:table_max_rows] for tb_col in table["columns"]]
+    )
+
+    num_cols, _ = tb.shape[0], tb.shape[1]
+
+    # if num_rows > table_max_rows:
+    #     tb = tb[:, np.random.choice(num_cols, table_max_rows, replace=False)]
+    #     num_cols = table_max_rows
+
+    anchor_row_num = tb.shape[1]
+    anchor_table = tb.reshape(-1)
+    anchor_table = tokenizer(
+        anchor_table.astype(str).tolist(),
+        padding="max_length",
+        truncation=True,
+        max_length=model_config.hf_config.encoder_config.encoder_max_length,
+        return_tensors="pt",
+    )
+    anchor_table = {
+        k: v.reshape(anchor_row_num, num_cols, -1)
+        for k, v in anchor_table.items()
+    }
+
+    num_cols = anchor_table["input_ids"].shape[1]
+
+    anchor_table_row_num = anchor_table["input_ids"].shape[0]
+
+    anchor_table_padded = {
+        k: F.pad(
+            v,
+            (
+                0,
+                0,
+                0,
+                table_max_cols - v.shape[1],
+                0,
+                table_max_rows - v.shape[0],
+            ),
+            "constant",
+            1,
+        )
+        for k, v in anchor_table.items()
+    }
+
+    anchor_table_mask = np.zeros((table_max_rows, table_max_cols))
+
+    anchor_table_mask[:anchor_table_row_num, :num_cols] = 1
+
+    ret = (
+        anchor_table_padded["input_ids"],
+        anchor_table_padded["attention_mask"],
+        anchor_table_padded["token_type_ids"],
+        torch.tensor(anchor_table_mask),
+        df_col_count,
+    )
+    return ret
+
+
+def get_encoder_output(
+    tables: t.List[ColumnsTable],
+    model_config: ModelConfig,
+    tokenizer: PreTrainedTokenizer,
+):
+    table_count = [len(tables)]
+
+    column_count = []
+    table_embeds = []
+    for table_list in [tables]:
+        anchor_table_input_ids = []
+        anchor_table_attention_mask = []
+        anchor_table_token_type_ids = []
+        anchor_table_mask = []
+        cur_column_count = []
+        for table in table_list:
+            p, q, r, s, cnt = get_embedded_table(table, model_config, tokenizer)
+            cur_column_count.append(cnt)
+            anchor_table_input_ids.append(p)
+            anchor_table_attention_mask.append(q)
+            anchor_table_token_type_ids.append(r)
+            anchor_table_mask.append(s)
+
+        column_count.append(cur_column_count)
+
+        anchor_table_input_ids = torch.stack(anchor_table_input_ids, dim=0).to(
+            device=TABGPT_ENCODER.st.device
+        )
+        anchor_table_attention_mask = torch.stack(
+            anchor_table_attention_mask, dim=0
+        ).to(device=TABGPT_ENCODER.st.device)
+        anchor_table_token_type_ids = torch.stack(
+            anchor_table_token_type_ids, dim=0
+        ).to(device=TABGPT_ENCODER.st.device)
+        anchor_table_mask = torch.stack(anchor_table_mask, dim=0).to(
+            device=TABGPT_ENCODER.st.device
+        )
+
+        table_embeds.append(
+            TABGPT_ENCODER(
+                anchor_table_input_ids,
+                anchor_table_attention_mask,
+                anchor_table_token_type_ids,
+                anchor_table_mask,
+            )
+        )
+        del (
+            anchor_table_input_ids,
+            anchor_table_attention_mask,
+            anchor_table_token_type_ids,
+            anchor_table_mask,
+        )
+
+    cat_table_embeds = [[] for _ in range(len(table_count))]
+    for i in range(len(table_count)):
+        for j in range(len(column_count[i])):
+            cat_table_embeds[i].append(table_embeds[i][j, : column_count[i][j]])
+        cat_table_embeds[i] = torch.cat(cat_table_embeds[i], dim=0)
+    return cat_table_embeds
+
+
+def input_processor_for_tablegpt_encoder(
+    ctx: InputContext, llm_inputs: LLMInputs
+):
+    hf_config = ctx.model_config.hf_config
+    if hf_config.encoder_config is None:
+        raise ValueError(
+            "Cannot found the table encoder config in the model hf config"
+        )
+
+    mm_data = llm_inputs["multi_modal_data"]
+
+    tokenizer = cached_get_tokenizer(
+        ctx.model_config.model,
+        subfolder=ctx.model_config.hf_config.encoder_config.subfolder,
+    )
+
+    table_embeddings = get_encoder_output(
+        mm_data["table"], model_config=ctx.model_config, tokenizer=tokenizer
+    )
+
+    return LLMInputs(
+        prompt_token_ids=llm_inputs["prompt_token_ids"],
+        multi_modal_data={"table": table_embeddings[0]},
+    )
+
+
+def get_table_max_cols_rows(
+    hf_config: TableGPTContrastiveConfig,
+) -> t.Tuple[int, int]:
+    max_rows = hf_config.encoder_config.max_rows
+    max_cols = hf_config.encoder_config.max_cols
+
+    return max_cols, max_rows
+
+
+def dummy_tabledata_for_contrastive_tablegpt(
+    model_config: ModelConfig,
+    table_max_rows: t.Optional[int] = None,
+    table_max_cols: t.Optional[int] = None,
+) -> t.Dict:
+    def generate_random_text(length: int) -> str:
+        """random string by length"""
+        return "".join(
+            random.choice(string.ascii_letters + string.digits)
+            for _ in range(length)
+        )
+
+    # random str set to col-row
+    df = pd.DataFrame(
+        {
+            f"col_{i}": [
+                generate_random_text(
+                    model_config.hf_config.encoder_config.encoder_max_length
+                )
+                for _ in range(table_max_rows)
+            ]
+            for i in range(table_max_cols)
+        }
+    )
+
+    # to table dict
+    table = [
+        {
+            "columns": [
+                {
+                    "name": df.columns[i],
+                    "dtype": str(df.dtypes[i]),
+                    "values": df[df.columns[i]].tolist(),
+                }
+                for i in range(len(df.columns))
+            ]
+        }
+    ]
+
+    # encoder here?
+    tokenizer = cached_get_tokenizer(
+        model_config.model,
+        subfolder=model_config.hf_config.encoder_config.subfolder,
+    )
+
+    table_embeddings = get_encoder_output(
+        table, model_config=model_config, tokenizer=tokenizer
+    )
+
+    return {"table": table_embeddings[0]}
+
+
+def dummy_seq_data_for_contrastive_tablegpt(
+    hf_config: TableGPTContrastiveConfig, seq_len: int
+):
+    encoder_config = hf_config.encoder_config
+
+    table_token_insert_id = encoder_config.insert_embs_token_id
+    encoder_table_max_col = encoder_config.max_cols
+
+    # this is the table placeholder tokens for contrastive(longlin) table encoder
+    token_ids = array(
+        VLLM_TOKEN_ID_ARRAY_TYPE,
+        [table_token_insert_id] * 3 * encoder_table_max_col,
+    )
+
+    # extend the token ids to max seq len
+    token_ids += array(VLLM_TOKEN_ID_ARRAY_TYPE, [0]) * (
+        seq_len - len(token_ids)
+    )
+
+    return SequenceData(token_ids)
+
+
+def dummy_data_for_contrastive_tablegpt(
+    ctx: InputContext, seq_len: int, mm_counts: t.Mapping[str, int]
+):
+    # num_tables = mm_counts["table"]
+    hf_config = ctx.model_config.hf_config
+
+    table_max_cols, table_max_rows = get_table_max_cols_rows(hf_config)
+
+    seq_data = dummy_seq_data_for_contrastive_tablegpt(hf_config, seq_len)
+
+    mm_data = dummy_tabledata_for_contrastive_tablegpt(
+        ctx.model_config, table_max_rows, table_max_cols
+    )
+
+    return seq_data, mm_data
+
+
+def exists(val):
+    return val is not None
+
+
+def default(val, d):
+    return val if exists(val) else d
+
+
+def ff_encodings(x, B):
+    x_proj = (2.0 * np.pi * x.unsqueeze(-1)) @ B.t()
+    return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)
+
+
+def mask_fill_value(dtype=torch.float16):
+    return torch.finfo(dtype).min if dtype == torch.float16 else float("-1e10")
+
+
+# classes
+
+
+class Residual(nn.Module):
+    def __init__(self, fn):
+        super().__init__()
+        self.fn = fn
+
+    def forward(self, x, **kwargs):
+        return self.fn(x, **kwargs) + x
+
+
+class PreNorm(nn.Module):
+    def __init__(self, dim, fn):
+        super().__init__()
+        self.norm = nn.LayerNorm(dim)
+        self.fn = fn
+
+    def forward(self, x, **kwargs):
+        return self.fn(self.norm(x), **kwargs)
+
+
+# attention
+
+
+class GEGLU(nn.Module):
+    def forward(self, x):
+        x, gates = x.chunk(2, dim=-1)
+        return x * F.gelu(gates)
+
+
+class FeedForward(nn.Module):
+    def __init__(self, dim, mult=4, dropout=0.0):
+        super().__init__()
+        self.net = nn.Sequential(
+            nn.Linear(dim, dim * mult * 2),
+            GEGLU(),
+            nn.Dropout(dropout),
+            nn.Linear(dim * mult, dim),
+        )
+
+    def forward(self, x, **kwargs):
+        return self.net(x)
+
+
+class RowColAttention(nn.Module):
+    def __init__(self, dim, heads=8, dim_head=16, dropout=0.0):
+        super().__init__()
+        inner_dim = dim_head * heads
+        self.heads = heads
+        self.scale = dim_head**-0.5
+
+        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)
+        self.to_out = nn.Linear(inner_dim, dim)
+
+        self.dropout = nn.Dropout(dropout)
+
+    def forward(self, x, mask=None):
+        h = self.heads
+        q, k, v = self.to_qkv(x).chunk(3, dim=-1)
+        # s = batch size
+        # b = number of rows
+        # h = number of heads
+        # n = number of columns
+        q, k, v = map(
+            lambda t: rearrange(t, "s b n (h d) -> s b h n d", h=h), (q, k, v)
+        )
+        sim = einsum("s b h i d, s b h j d -> s b h i j", q, k) * self.scale
+
+        # masking
+        if mask is not None:
+            mask = (
+                mask.unsqueeze(1)
+                .unsqueeze(1)
+                .repeat(1, sim.shape[1], sim.shape[2], 1, 1)
+            )
+            sim = sim.masked_fill(mask == 0, mask_fill_value(sim.dtype))
+
+        attn = sim.softmax(dim=-1)
+        out = einsum("s b h i j, s b h j d -> s b h i d", attn, v)
+        out = rearrange(out, "s b h n d -> s b n (h d)", h=h)
+        return self.to_out(out)
+
+
+class Attention(nn.Module):
+    def __init__(self, dim, heads=8, dim_head=16, dropout=0.0):
+        super().__init__()
+        inner_dim = dim_head * heads
+        self.heads = heads
+        self.scale = dim_head**-0.5
+
+        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)
+        self.to_out = nn.Linear(inner_dim, dim)
+
+        self.dropout = nn.Dropout(dropout)
+
+    def forward(self, x, mask=None):
+        h = self.heads
+        q, k, v = self.to_qkv(x).chunk(3, dim=-1)
+        # s = batch size
+        # b = number of rows
+        # h = number of heads
+        # n = number of columns
+        q, k, v = map(
+            lambda t: rearrange(t, "b n (h d) -> b h n d", h=h), (q, k, v)
+        )
+
+        sim = einsum("b h i d, b h j d -> b h i j", q, k) * self.scale
+
+        # masking
+        # torch.Size([12, 300, 300])
+        if mask is not None:
+            mask = mask.unsqueeze(1).repeat(1, sim.shape[1], 1, 1)
+            sim = sim.masked_fill(mask == 0, mask_fill_value(sim.dtype))
+
+        attn = sim.softmax(dim=-1)
+        out = einsum("b h i j, b h j d -> b h i d", attn, v)
+        out = rearrange(out, "b h n d -> b n (h d)", h=h)
+        return self.to_out(out)
+
+
+class Qformer(nn.Module):
+    def __init__(self, dim, dim_head, inner_dim, query_num):
+        super().__init__()
+
+        self.heads = inner_dim // dim_head
+        self.query_num = query_num
+        self.scale = dim_head**-0.5
+        self.q = nn.Parameter(torch.randn(query_num, inner_dim))
+        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)
+        self.ff = PreNorm(inner_dim, Residual(FeedForward(inner_dim)))
+
+    def forward(self, x, mask=None):
+        x = rearrange(x, "s b n d -> s n b d")
+
+        h = self.heads
+        k, v = self.to_kv(x).chunk(2, dim=-1)
+        q = (
+            self.q.unsqueeze(0)
+            .unsqueeze(0)
+            .repeat(x.shape[0], x.shape[1], 1, 1)
+        )
+        q, k, v = map(
+            lambda t: rearrange(t, "s b n (h d) -> s b h n d", h=h), (q, k, v)
+        )
+        sim = einsum("s b h i d, s b h j d -> s b h i j", q, k) * self.scale
+
+        # masking
+        if mask is not None:
+            mask = rearrange(mask, "s i j -> s j i")
+            mask = (
+                mask[:, 0, :]
+                .unsqueeze(1)
+                .unsqueeze(1)
+                .unsqueeze(1)
+                .repeat(1, sim.shape[1], sim.shape[2], sim.shape[3], 1)
+            )
+            sim = sim.masked_fill(mask == 0, mask_fill_value(sim.dtype))
+
+        attn = sim.softmax(dim=-1)
+        out = einsum("s b h i j, s b h j d -> s b h i d", attn, v)
+        out = rearrange(out, "s b h n d -> s b n (h d)", h=h)
+
+        out = self.ff(out)
+        return out
+
+
+class RowColTransformer(nn.Module):
+    # dim = dim of each token
+    # nfeats = number of features (columns)
+    # depth = number of attention layers
+    # heads = number of heads in multihead attention
+    # dim_head = dim of each head
+    def __init__(
+        self,
+        dim,
+        nfeats,
+        depth,
+        heads,
+        dim_head,
+        attn_dropout,
+        ff_dropout,
+        style="col",
+        mask=None,
+    ):
+        super().__init__()
+        self.layers = nn.ModuleList([])
+        self.style = style
+
+        for _ in range(depth):
+            if self.style == "colrow":
+                self.layers.append(
+                    nn.ModuleList(
+                        [
+                            PreNorm(
+                                dim,
+                                Residual(
+                                    RowColAttention(
+                                        dim,
+                                        heads=heads,
+                                        dim_head=dim_head,
+                                        dropout=attn_dropout,
+                                    )
+                                ),
+                            ),
+                            PreNorm(
+                                dim,
+                                Residual(FeedForward(dim, dropout=ff_dropout)),
+                            ),
+                            PreNorm(
+                                dim,
+                                Residual(
+                                    RowColAttention(
+                                        dim,
+                                        heads=heads,
+                                        dim_head=dim_head,
+                                        dropout=attn_dropout,
+                                    )
+                                ),
+                            ),
+                            PreNorm(
+                                dim,
+                                Residual(FeedForward(dim, dropout=ff_dropout)),
+                            ),
+                        ]
+                    )
+                )
+            else:
+                self.layers.append(
+                    nn.ModuleList(
+                        [
+                            PreNorm(
+                                dim * nfeats,
+                                Residual(
+                                    Attention(
+                                        dim * nfeats,
+                                        heads=heads,
+                                        dim_head=64,
+                                        dropout=attn_dropout,
+                                    )
+                                ),
+                            ),
+                            PreNorm(
+                                dim * nfeats,
+                                Residual(
+                                    FeedForward(
+                                        dim * nfeats, dropout=ff_dropout
+                                    )
+                                ),
+                            ),
+                        ]
+                    )
+                )
+
+    def forward(self, x, mask=None):
+        _, _, n, _ = x.shape  # [bs, n_rows, n_cols, dim]
+        row_mask = None
+        col_mask = None
+        if mask is not None:
+            col_mask = einsum("b i j, b i k -> b j k", mask, mask)
+            row_mask = einsum("b i j, b k j -> b i k", mask, mask)
+        # print(col_mask.shape, row_mask.shape)
+        if self.style == "colrow":
+            for attn1, ff1, attn2, ff2 in self.layers:
+                x = attn1(x, mask=col_mask)
+                x = ff1(x)
+                x = rearrange(x, "s b n d -> s n b d")
+                x = attn2(x, mask=row_mask)
+                x = ff2(x)
+                x = rearrange(x, "s n b d -> s b n d", n=n)
+        else:
+            for attn1, ff1 in self.layers:
+                x = rearrange(x, "s b n d -> s 1 b (n d)")
+                x = attn1(x)
+                x = ff1(x)
+                x = rearrange(x, "s 1 b (n d) -> s b n d", n=n)
+        return x
+
+
+# transformer
+class Transformer(nn.Module):
+    def __init__(self, dim, depth, heads, dim_head, attn_dropout, ff_dropout):
+        super().__init__()
+        self.layers = nn.ModuleList([])
+
+        for _ in range(depth):
+            self.layers.append(
+                nn.ModuleList(
+                    [
+                        PreNorm(
+                            dim,
+                            Residual(
+                                Attention(
+                                    dim,
+                                    heads=heads,
+                                    dim_head=dim_head,
+                                    dropout=attn_dropout,
+                                )
+                            ),
+                        ),
+                        PreNorm(
+                            dim, Residual(FeedForward(dim, dropout=ff_dropout))
+                        ),
+                    ]
+                )
+            )
+
+    def forward(self, x, mask=None):
+        for attn, ff in self.layers:
+            x = attn(x, mask=mask)
+            x = ff(x)
+        return x
+
+
+# mlp
+class MLP(nn.Module):
+    def __init__(self, dims, act=None):
+        super().__init__()
+        dims_pairs = list(zip(dims[:-1], dims[1:]))
+        layers = []
+        for ind, (dim_in, dim_out) in enumerate(dims_pairs):
+            is_last = ind >= (len(dims) - 1)
+            linear = nn.Linear(dim_in, dim_out)
+            layers.append(linear)
+
+            if is_last:
+                continue
+            if act is not None:
+                layers.append(act)
+
+        self.mlp = nn.Sequential(*layers)
+
+    def forward(self, x):
+        return self.mlp(x)
+
+
+class simple_MLP(nn.Module):
+    def __init__(self, dims):
+        super(simple_MLP, self).__init__()
+        self.layers = nn.Sequential(
+            nn.Linear(dims[0], dims[1]), nn.ReLU(), nn.Linear(dims[1], dims[2])
+        )
+
+    def forward(self, x):
+        if len(x.shape) == 1:
+            x = x.view(x.size(0), -1)
+        x = self.layers(x)
+        return x
+
+
+def get_flatten_table_emb(table_emb, mask):
+    flatten_table_emb = torch.zeros(
+        table_emb.size(0), table_emb.size(2), table_emb.size(3)
+    ).to(table_emb.device)
+    row_num = torch.sum(mask, dim=1).int()
+    for i in range(len(table_emb)):
+        flatten_table_emb[i] = torch.mean(
+            table_emb[i, : row_num[i, 0], :, :], dim=0
+        )
+    return flatten_table_emb
+
+
+# helpers
+class sep_MLP(nn.Module):
+    def __init__(self, dim, len_feats, categories):
+        super(sep_MLP, self).__init__()
+        self.len_feats = len_feats
+        self.layers = nn.ModuleList([])
+        for i in range(len_feats):
+            self.layers.append(simple_MLP([dim, 5 * dim, categories[i]]))
+
+    def forward(self, x):
+        y_pred = list([])
+        for i in range(self.len_feats):
+            x_i = x[:, i, :]
+            pred = self.layers[i](x_i)
+            y_pred.append(pred)
+        return y_pred
+
+
+class TableEncoder(nn.Module):
+    def __init__(self, config: PretrainedConfig, **kwargs):
+        super().__init__()
+        self.config = config
+
+        encoder_config = config.encoder_config
+
+        self.num_cols = encoder_config.num_cols
+        self.attentiontype = encoder_config.attentiontype
+        # self.final_mlp_style = encoder_config.final_mlp_style
+        self.pred_type = encoder_config.pred_type
+        self.cont_dim = encoder_config.cont_dim
+        self.pooling = encoder_config.pooling
+        self.numeric_mlp = encoder_config.numeric_mlp
+        self.ff_dropout = encoder_config.ff_dropout
+        self.attn_dropout = encoder_config.attn_dropout
+        self.dim_head = encoder_config.dim_head
+        self.depth = encoder_config.depth
+        self.heads = encoder_config.heads
+
+        self.st = AutoModel.from_config(BertConfig(**encoder_config.st_config))
+        self.dim = self.st.config.hidden_size
+
+        self.st.pooler = None
+
+        # transformer
+        self.transformer = RowColTransformer(
+            dim=self.dim,
+            nfeats=self.num_cols,
+            depth=self.depth,
+            heads=self.heads,
+            dim_head=self.dim_head,
+            attn_dropout=self.attn_dropout,
+            ff_dropout=self.ff_dropout,
+            style=self.attentiontype,
+        )
+
+        self.col_specific_projection_head = simple_MLP(
+            [self.dim, self.dim, self.cont_dim]
+        )
+
+        self.qformer = Qformer(
+            dim=self.dim, dim_head=128, inner_dim=3584, query_num=3
+        )
+
+    # Mean Pooling - Take attention mask into account for correct averaging
+    def mean_pooling(self, model_output, attention_mask):
+        token_embeddings = model_output[
+            0
+        ]  # First element of model_output contains all token embeddings
+
+        input_mask_expanded = (
+            attention_mask.unsqueeze(-1)
+            .expand(token_embeddings.size())
+            .to(dtype=token_embeddings.dtype)
+        )
+
+        return torch.sum(
+            token_embeddings * input_mask_expanded, 1
+        ) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
+
+    def get_embeddings(self, input_ids, attention_mask, token_type_ids):
+        bs, num_rows, num_cols, seq_len = (
+            input_ids.shape[0],
+            input_ids.shape[1],
+            input_ids.shape[2],
+            input_ids.shape[3],
+        )
+        input_ids = input_ids.reshape(-1, seq_len)
+        attention_mask = attention_mask.reshape(-1, seq_len)
+        if token_type_ids is not None:
+            token_type_ids = token_type_ids.reshape(-1, seq_len)
+
+        last_hidden_state = self.st(
+            input_ids=input_ids,
+            attention_mask=attention_mask,
+            token_type_ids=token_type_ids,
+        )
+        embeddings = self.mean_pooling(last_hidden_state, attention_mask)
+        embeddings = F.normalize(embeddings, p=2, dim=-1)
+
+        embeddings = embeddings.reshape(bs, num_rows, num_cols, -1)
+
+        return embeddings
+
+    @torch.inference_mode()
+    def forward(
+        self,
+        input_ids,
+        attention_mask,
+        token_type_ids,
+        table_mask,
+    ):
+        tab_emb = self.get_embeddings(input_ids, attention_mask, token_type_ids)
+
+        if self.pooling == "cls":
+            # roll the table on dim 1 (row dim)
+            tab_emb = torch.roll(tab_emb, 1, 1)
+            # insert [cls] token at the first row
+            tab_emb[:, 0, :, :] = self.cls
+
+        cell_emb = self.transformer(tab_emb, mask=table_mask)
+
+        col_emb = self.attn_pooling(cell_emb, table_mask)
+
+        return col_emb
+
+    def attn_pooling(self, cell_emb, table_mask):
+        output = self.qformer(cell_emb, mask=table_mask)
+        return output
diff --git a/vllm/multimodal/base.py b/vllm/multimodal/base.py
index 8ada60c8..fdc75541 100644
--- a/vllm/multimodal/base.py
+++ b/vllm/multimodal/base.py
@@ -3,14 +3,14 @@ from abc import ABC, abstractmethod
 from collections import UserDict, defaultdict
 from typing import Callable, Dict, List, Mapping, Optional
 from typing import Sequence as GenericSequence
-from typing import Tuple, Type, TypedDict, TypeVar, Union, cast, final
+from typing import Tuple, Type, TypeVar, Union, cast, final
 
 import numpy as np
 import torch
 import torch.types
 from PIL import Image
 from torch import nn
-from typing_extensions import TypeAlias
+from typing_extensions import Required, TypeAlias, TypedDict
 
 from vllm.config import ModelConfig
 from vllm.inputs import InputContext
@@ -127,6 +127,22 @@ The number of data instances allowed per modality is restricted by
 """
 
 
+class TableCol(TypedDict, total=False):
+
+    name: Required[str]
+    dtype: Required[str]
+
+    values: Required[List]
+
+    contains_nan = False
+    is_unique = False
+
+
+class ColumnsTable(TypedDict, total=False):
+    columns: Required[List[TableCol]]
+
+MarkdownTable: TypeAlias = str
+
 @final
 class MultiModalDataBuiltins(TypedDict, total=False):
     """Modality types that are predefined by vLLM."""
@@ -137,6 +153,9 @@ class MultiModalDataBuiltins(TypedDict, total=False):
     audio: MultiModalData[Tuple[np.ndarray, Union[int, float]]]
     """The input audio item(s) and corresponding sampling rate(s)."""
 
+    table: MultiModalData[Union[List[ColumnsTable]| List[MarkdownTable]]]
+    """The input table column(s) and corresponding sampling rate(s)."""
+
 
 MultiModalDataDict = Union[MultiModalDataBuiltins,
                            Mapping[str, MultiModalData[object]]]
diff --git a/vllm/multimodal/registry.py b/vllm/multimodal/registry.py
index cd16cdcb..a73cb952 100644
--- a/vllm/multimodal/registry.py
+++ b/vllm/multimodal/registry.py
@@ -9,6 +9,7 @@ from .audio import AudioPlugin
 from .base import (MultiModalDataDict, MultiModalInputMapper, MultiModalInputs,
                    MultiModalPlugin, MultiModalTokensCalc, NestedTensors)
 from .image import ImagePlugin
+from .table import TablePlugin
 
 logger = init_logger(__name__)
 
@@ -34,7 +35,7 @@ class MultiModalRegistry:
     :class:`~vllm.multimodal.MultiModalPlugin` for each modality.
     """
 
-    DEFAULT_PLUGINS = (ImagePlugin(), AudioPlugin())
+    DEFAULT_PLUGINS = (ImagePlugin(), AudioPlugin(), TablePlugin())
 
     def __init__(
             self,
@@ -95,6 +96,17 @@ class MultiModalRegistry:
         """
         return self.register_input_mapper("image", mapper)
 
+    def register_table_input_mapper(
+        self,
+        mapper: Optional[MultiModalInputMapper] = None,
+    ):
+        """
+        Register an input mapper for table data to a model class.
+
+        See :meth:`MultiModalPlugin.register_input_mapper` for more details.
+        """
+        return self.register_input_mapper("table", mapper)
+
     def map_input(self, model_config: ModelConfig,
                   data: MultiModalDataDict) -> MultiModalInputs:
         """
@@ -162,6 +174,7 @@ class MultiModalRegistry:
         """
         return self.register_max_multimodal_tokens("image", max_mm_tokens)
 
+
     def get_max_multimodal_tokens(self, model_config: ModelConfig) -> int:
         """
         Get the maximum number of multi-modal tokens
diff --git a/vllm/multimodal/table.py b/vllm/multimodal/table.py
new file mode 100644
index 00000000..b079a468
--- /dev/null
+++ b/vllm/multimodal/table.py
@@ -0,0 +1,29 @@
+import torch
+
+from vllm.inputs.registry import InputContext
+from vllm.logger import init_logger
+
+from .base import MultiModalInputs, MultiModalPlugin
+
+
+logger = init_logger(__name__)
+
+
+class TablePlugin(MultiModalPlugin):
+    """Plugin for table data."""
+
+    def get_data_key(self) -> str:
+        return "table"
+
+    def _default_input_mapper(self, ctx: InputContext,
+                              data: object) -> MultiModalInputs:
+        if not isinstance(data, torch.Tensor):
+            raise ValueError(f"Table data must be a tensor, got {type(data)}")
+
+        data = data.to(dtype=ctx.model_config.dtype)
+        return MultiModalInputs({"table": data})
+
+    def _default_max_multimodal_tokens(self, ctx: InputContext) -> int:
+        raise NotImplementedError(
+            "There is no default maximum multimodal tokens")
+
diff --git a/vllm/transformers_utils/config.py b/vllm/transformers_utils/config.py
index c2276b07..81a6fb61 100644
--- a/vllm/transformers_utils/config.py
+++ b/vllm/transformers_utils/config.py
@@ -15,6 +15,8 @@ from vllm.transformers_utils.configs import (ChatGLMConfig, DbrxConfig,
                                              JAISConfig, MedusaConfig,
                                              MLPSpeculatorConfig, MPTConfig,
                                              NemotronConfig, RWConfig,
+                                             TableGPTContrastiveConfig, 
+                                             TableGPTMarkUpConfig,
                                              UltravoxConfig)
 
 if VLLM_USE_MODELSCOPE:
@@ -36,6 +38,8 @@ _CONFIG_REGISTRY: Dict[str, Type[PretrainedConfig]] = {
     "eagle": EAGLEConfig,
     "internvl_chat": InternVLChatConfig,
     "nemotron": NemotronConfig,
+    "tablegpt_contrastive": TableGPTContrastiveConfig,
+    "tablegpt_markup":TableGPTMarkUpConfig,
     "ultravox": UltravoxConfig,
 }
 
diff --git a/vllm/transformers_utils/configs/__init__.py b/vllm/transformers_utils/configs/__init__.py
index dc2fd6a8..fe579001 100644
--- a/vllm/transformers_utils/configs/__init__.py
+++ b/vllm/transformers_utils/configs/__init__.py
@@ -11,6 +11,7 @@ from vllm.transformers_utils.configs.medusa import MedusaConfig
 from vllm.transformers_utils.configs.mlp_speculator import MLPSpeculatorConfig
 from vllm.transformers_utils.configs.mpt import MPTConfig
 from vllm.transformers_utils.configs.nemotron import NemotronConfig
+from vllm.transformers_utils.configs.tablegpt import TableGPTContrastiveConfig, TableGPTMarkUpConfig
 from vllm.transformers_utils.configs.ultravox import UltravoxConfig
 
 __all__ = [
@@ -24,5 +25,7 @@ __all__ = [
     "EAGLEConfig",
     "MLPSpeculatorConfig",
     "NemotronConfig",
+    "TableGPTContrastiveConfig",
+    "TableGPTMarkUpConfig",
     "UltravoxConfig",
 ]
diff --git a/vllm/transformers_utils/configs/tablegpt.py b/vllm/transformers_utils/configs/tablegpt.py
new file mode 100644
index 00000000..dfbb2e22
--- /dev/null
+++ b/vllm/transformers_utils/configs/tablegpt.py
@@ -0,0 +1,105 @@
+from transformers.configuration_utils import PretrainedConfig
+from transformers.utils import logging
+
+logger = logging.get_logger(__name__)
+
+
+class TableGPTContrastiveConfig(PretrainedConfig):
+    model_type = 'tablegpt_contrastive'
+    is_composition = True
+
+    def __init__(self,
+                 encoder_config=None,
+                 llm_config=None,
+                 projector_config=None,
+                 **kwargs):
+
+        super().__init__(**kwargs)
+
+        self.encoder_config = PretrainedConfig(**encoder_config)
+        self.projector_config = PretrainedConfig(**projector_config)
+        self.text_config = PretrainedConfig(**llm_config)
+
+class CodeT5pModuleConfig(PretrainedConfig):
+    model_type = "codet5p_module"
+    attribute_map = {
+        "max_position_embeddings": "n_positions",
+        "hidden_size": "n_embd",
+        "num_attention_heads": "n_head",
+        "num_hidden_layers": "n_layer",
+    }
+
+    def __init__(
+            self,
+            vocab_size=50400,
+            n_positions=2048,
+            n_ctx=2048,
+            n_embd=4096,
+            n_layer=28,
+            n_head=16,
+            rotary_dim=64,
+            n_inner=None,
+            activation_function="gelu_new",
+            resid_pdrop=0.0,
+            embd_pdrop=0.0,
+            attn_pdrop=0.0,
+            layer_norm_epsilon=1e-5,
+            initializer_range=0.02,
+            scale_attn_weights=True,
+            use_cache=True,
+            bos_token_id=50256,
+            eos_token_id=50256,
+            tie_word_embeddings=False,
+            **kwargs
+    ):
+        self.vocab_size = vocab_size
+        self.n_ctx = n_ctx
+        self.n_positions = n_positions
+        self.n_embd = n_embd
+        self.n_layer = n_layer
+        self.n_head = n_head
+        self.n_inner = n_inner
+        self.rotary_dim = rotary_dim
+        self.activation_function = activation_function
+        self.resid_pdrop = resid_pdrop
+        self.embd_pdrop = embd_pdrop
+        self.attn_pdrop = attn_pdrop
+        self.layer_norm_epsilon = layer_norm_epsilon
+        self.initializer_range = initializer_range
+        self.scale_attn_weights = scale_attn_weights
+        self.use_cache = use_cache
+
+        self.bos_token_id = bos_token_id
+        self.eos_token_id = eos_token_id
+
+        super().__init__(
+            bos_token_id=bos_token_id, eos_token_id=eos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs
+        )
+
+
+class TableGPTMarkUpConfig(PretrainedConfig):
+    model_type = 'tablegpt_markup'
+    is_composition = True
+
+    def __init__(self,
+                 encoder_config=None,
+                 llm_config=None,
+                 encoder_hidden_size=1024,
+                 decoder_hidden_size=3584,
+                 mlp_depth=1,
+                 encoder_max_length=64,
+                 placeholder_token="<TABLE_CONTNET>",
+                 placeholder_token_id=-114,
+                 **kwargs):
+
+        super().__init__(**kwargs)
+
+        self.encoder_config = CodeT5pModuleConfig(**encoder_config)
+        self.text_config = PretrainedConfig(**llm_config)
+
+        self.encoder_hidden_size = encoder_hidden_size
+        self.decoder_hidden_size = decoder_hidden_size
+        self.mlp_depth = mlp_depth
+        self.encoder_max_length = encoder_max_length
+        self.placeholder_token= placeholder_token
+        self.placeholder_token_id = placeholder_token_id
